{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nxseed = 10\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "np.random.seed(5)\n",
    "torch.manual_seed(5)\n",
    "if device is not \"cpu\":\n",
    "    torch.cuda.manual_seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trim_edges(a, b):\n",
    "#     g_a = nx.Graph(a)\n",
    "#     g_b = nx.Graph(b)\n",
    "#     print(g_a.degree(), \"\\n\", g_b.degree())\n",
    "#     for node in range(len(g_a.degree)):\n",
    "#         print(\"Node :\", node)\n",
    "#         print(g_a.edges(node))\n",
    "#         print(g_b.edges(node))\n",
    "#         diff = len(g_a.edges(node)) - len(g_b.edges(node))\n",
    "#         g_aedges = g_a.edges(node)\n",
    "#         g_bedges = g_b.edges(node)\n",
    "\n",
    "#         temp = np.arange(len(g_a.edges(node)))\n",
    "#         np.random.shuffle(temp)\n",
    "#         print(\"Diff is :\", diff)\n",
    "#         for i in range(diff):\n",
    "#             print(\"Node :\", node, \"Edge :\", temp[i])\n",
    "#             g_a.remove_edge(node, list(g_aedges)[temp[i]][1])   \n",
    "#     return nx.adjacency_matrix(g_a).todense()\n",
    "\n",
    "# def generate_trimmed_graphs(graphadjs_a, graphadjs_b):\n",
    "#     new_graphadjs_a = []\n",
    "#     for graphadj_a, graphadj_b in zip(graphadjs_a, graphadjs_b):\n",
    "#         new_graphadjs_a.append(trim_edges(graphadj_a, graphadj_b))\n",
    "#     return graphadjs_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incidence(a):\n",
    "    \"\"\"make incidence matrix from adjacency matrix\"\"\"\n",
    "    ix = np.where(np.triu(a))\n",
    "    edg = array(list(zip(*ix)))\n",
    "    incid = np.zeros((len(a), len(edg)), dtype=np.float32)\n",
    "    for _,[i,j] in enumerate(edg): \n",
    "        incid[i,_] = 1\n",
    "        incid[j,_] = -1\n",
    "    return incid\n",
    "\n",
    "def incidence_shuffle(incid):\n",
    "    neg = np.where(incid < 0)\n",
    "    # preserve the rows, to preserve node degree, but shuffle columns, to shuffle links\n",
    "    np.random.shuffle(neg[1])\n",
    "    incid2 = 1.* (incid> 0 )\n",
    "    incid2[neg] -= 1\n",
    "    return incid2\n",
    "\n",
    "def configuration_model_1(a, weighted = True):\n",
    "    incid = get_incidence(a)\n",
    "    incid2 = incidence_shuffle(incid)\n",
    "    lap = incid2.dot(incid2.T)\n",
    "    deg = lap.diagonal() \n",
    "    a2 = np.diag(deg)-lap\n",
    "    if not weighted:\n",
    "        a2 = (1*(a2>0))\n",
    "    return a2\n",
    "\n",
    "def trim_mat(a,b):\n",
    "    \"\"\"remove random links from 'a' until it has the same number of links as 'b'\"\"\"\n",
    "    i = (a.sum(0)!= b.sum(0))\n",
    "    ms = i[:,np.newaxis]*i[np.newaxis]\n",
    "    masked_a = a*np.triu(ms)\n",
    "    ix = np.where(masked_a)\n",
    "    # num missing links\n",
    "    d = int(a.sum()-b.sum())//2\n",
    "    # randomly choose r indices to knockout\n",
    "    r = np.argsort(np.random.rand(len(ix[0])))[:d]\n",
    "    # make trimmed graph\n",
    "    c = triu(a) + 0.\n",
    "    for i in r:\n",
    "        c[ix[0][i],ix[1][i]]=0\n",
    "    c += c.T \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphData():\n",
    "    def __init__(self, num, nodes):\n",
    "        self.num = num\n",
    "        self.nodes = nodes\n",
    "        self.m_list = [1.0, nodes/8.0, nodes/4.0, 3.0*nodes/8.0, nodes/2.0]\n",
    "        self.p_list = np.linspace(1.0/num, num/2.0, 4)\n",
    "        \n",
    "        self.graphs = []\n",
    "        self.graphlabels = []\n",
    "        \n",
    "        self.graphadjs = []\n",
    "        \n",
    "        self.graphadjs_da = []\n",
    "        self.graphadjs_dad = []\n",
    "        \n",
    "        self.tokenisedgraphlabels = []\n",
    "    \n",
    "    #Generates [num] of graphs of type BA. Also add labels \"BA\"\n",
    "    def generate_BAGraphs(self):\n",
    "        num_each_group = int(self.num/len(self.m_list))\n",
    "        \n",
    "        for m in self.m_list:\n",
    "            for i in range(num_each_group):\n",
    "                graph = nx.barabasi_albert_graph(self.nodes, int(m))\n",
    "                self.graphs.append(graph)\n",
    "                self.graphlabels.append(\"BA\")\n",
    "                  \n",
    "    #Generates [num] of graphs of type ER. Also add labels \"ER\"\n",
    "    def generate_ERGraphs(self):\n",
    "        num_each_group = int(self.num/len(self.p_list))\n",
    "        \n",
    "        for p in self.p_list:\n",
    "            for i in range(num_each_group):\n",
    "                graph = nx.erdos_renyi_graph(self.nodes, p, directed=False)\n",
    "                self.graphs.append(graph)\n",
    "                self.graphlabels.append(\"ER\")\n",
    "                \n",
    "    #Generate [num] of graphs of type configurational from the graphs already built. Also add labels \"CG\"\n",
    "    def generate_configurational_graphs(self):\n",
    "        temp_graphs = self.graphs.copy()\n",
    "        for graph in temp_graphs:\n",
    "            degree = list(dict(graph.degree()).values())\n",
    "            graph_con = nx.configuration_model(degree)\n",
    "            graph_con = nx.Graph(graph_con)\n",
    "            \n",
    "            self.graphs.append(graph_con)\n",
    "            self.graphlabels.append(\"CG\")            \n",
    "                \n",
    "    \n",
    "    #Generate adjacent matrix from the generated graphs.\n",
    "    def generate_adjs(self):\n",
    "        for graph in self.graphs:\n",
    "            self.graphadjs.append(nx.adjacency_matrix(graph).todense())\n",
    "            \n",
    "    #Normalise the graphs for training purposes.\n",
    "    def normalise(self):\n",
    "        for x in self.graphadjs:\n",
    "            rowsum = np.array(x.sum(1), dtype = np.float32)\n",
    "            r_inv = np.power(rowsum, -1).flatten()\n",
    "            r_inv[np.isinf(r_inv)] = 0.\n",
    "            d = np.diag(r_inv)\n",
    "            da = d.dot(x)\n",
    "            d = np.sqrt(d)\n",
    "            dad = (d.dot(x)).dot(d)\n",
    "            self.graphadjs_da.append(da)\n",
    "            self.graphadjs_dad.append(dad)\n",
    "     \n",
    "    #Generate tokenised graph labels from the labels for training purposes.\n",
    "    def tokenise(self, map_dict):\n",
    "        self.tokenisedgraphlabels = list(map(map_dict.get, self.graphlabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(x, n_classes):\n",
    "    return np.eye(n_classes)[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    def train_step(x, h, y):\n",
    "        model.train()\n",
    "        yhat = model(h, x)\n",
    "        loss = loss_fn(yhat, y.long().squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss.item()\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(x_h, y, batch_size, ratio_train):\n",
    "\n",
    "    x_tensor = torch.from_numpy(x_h[0]).float()\n",
    "    h_tensor = torch.from_numpy(x_h[1]).float()\n",
    "    y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "    dataset = TensorDataset(x_tensor, h_tensor, y_tensor)\n",
    "    trainlength = int(ratio_train*x_h[0].shape[0])\n",
    "    vallength = x_h[0].shape[0] - trainlength\n",
    "    \n",
    "    train_dataset, val_dataset = random_split(dataset, [trainlength, vallength])\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size = batch_size)\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size = batch_size)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, train_step, n_epochs, train_loader, val_loader):\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        batch_losses = []\n",
    "        for x_batch, h_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            h_batch = h_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            loss = train_step(x_batch, h_batch, y_batch)\n",
    "            batch_losses.append(loss)\n",
    "        training_loss = np.mean(batch_losses)\n",
    "        training_losses.append(training_loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            for x_val, h_val, y_val in val_loader:\n",
    "                x_val = x_val.to(device)\n",
    "                h_val = h_val.to(device)\n",
    "                y_val = y_val.to(device)\n",
    "                model.eval()\n",
    "                \n",
    "                yhat = model(h_val, x_val)\n",
    "#                 print(y_val.shape)\n",
    "#                 print(yhat.shape)\n",
    "#                 yhat = (yhat >.5).float()\n",
    "#                 print(\"y_val\", y_val)\n",
    "#                 print(\"yhat\", torch.argmax(yhat, axis = 1).float())\n",
    "        \n",
    "                val_acc = (torch.tensor(y_val).view(-1).float() == torch.argmax(yhat, axis = 1).float()).float().sum()/(y_val.shape[0])\n",
    "                val_loss = loss_fn(yhat, y_val.long().squeeze())\n",
    "                print(\"Val Accuracy is:\", val_acc.item())\n",
    "                val_losses.append(val_loss.item())\n",
    "#                 print(dir(val_loss))\n",
    "            validation_loss = np.mean(val_losses)\n",
    "            validation_losses.append(validation_loss)\n",
    "\n",
    "        print(f\"[{epoch+1}] Training loss: {training_loss:.3f}\\t Validation loss: {validation_loss:.3f}\")\n",
    "    return training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_mat(a,b):\n",
    "    i = np.array(a.sum(0)!= b.sum(0))[0]\n",
    "    print(i)\n",
    "    ms = i[:,np.newaxis]*i[np.newaxis]\n",
    "    masked_a = a*np.triu(ms)\n",
    "    ix = np.where(masked_a)\n",
    "    d = int(a.sum()-b.sum())//2\n",
    "    r = np.argsort(np.random.rand(len(ix[0])))[:d]\n",
    "    c = np.triu(a) + 0.\n",
    "    for i in r:\n",
    "        c[ix[0][i],ix[1][i]]=0\n",
    "    c += c.T \n",
    "    return c\n",
    "\n",
    "def trim_graphadjs(graphadjs_a, graphadjs_b):\n",
    "    graphadjs_a_new = []\n",
    "    for a, b in zip(graphadjs_a, graphadjs_b):\n",
    "#         print(\"a\", a)\n",
    "#         print(\"b\", b)\n",
    "        graphadjs_a_new.append(trim_mat(a, b))\n",
    "    return graphadjs_a_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_graphadjs(graphadjs):\n",
    "    graphadjsshuffled = []\n",
    "    for adj in graphadjs:\n",
    "        idx = np.arange(adj.shape[0])\n",
    "        np.random.shuffle(idx)\n",
    "        graphadjsshuffled.append(adj[idx][:,idx])\n",
    "    return graphadjsshuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the map dataset for BA vs CG\n",
    "\n",
    "map_dict = {\"BA\" : 1, \"CG\": 0}\n",
    "n_classes = len(map_dict)\n",
    "\n",
    "graphdatabase = GraphData(2500, 50)\n",
    "graphdatabase.generate_BAGraphs()\n",
    "graphdatabase.generate_configurational_graphs()\n",
    "graphdatabase.generate_adjs()\n",
    "graphdatabase.normalise()\n",
    "graphdatabase.tokenise(map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graphdatabase.graphadjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = graphdatabase.graphadjs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = graphdatabase.graphadjs[2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True False False False False  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False]\n"
     ]
    }
   ],
   "source": [
    "c = trim_mat(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 3, 12,  6,  3,  1,  2,  5,  2,  3,  2,  2,  3,  3,  2,  3,  2,\n",
       "          2,  1,  2,  1,  1,  3,  5,  1,  1,  1,  2,  2,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1]], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3., 12.,  6.,  3.,  1.,  2.,  5.,  2.,  3.,  2.,  2.,  3.,  3.,\n",
       "        2.,  3.,  2.,  2.,  1.,  2.,  1.,  1.,  3.,  5.,  1.,  1.,  1.,\n",
       "        2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 3, 10,  6,  3,  1,  2,  4,  2,  3,  2,  2,  3,  3,  2,  3,  2,\n",
       "          2,  1,  2,  1,  1,  3,  5,  1,  1,  1,  2,  2,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1]], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_mat(a,b):\n",
    "    i = np.array(a.sum(0)!= b.sum(0))[0]\n",
    "    print(i)\n",
    "    ms = i[:,np.newaxis]*i[np.newaxis]\n",
    "    masked_a = a*np.triu(ms)\n",
    "    ix = np.where(masked_a)\n",
    "    d = int(a.sum()-b.sum())//2\n",
    "    r = np.argsort(np.random.rand(len(ix[0])))[:d]\n",
    "    c = np.triu(a) + 0.\n",
    "    for i in r:\n",
    "        c[ix[0][i],ix[1][i]]=0\n",
    "    c += c.T \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(a)\n",
    "b = np.array(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 12,  6,  3,  1,  2,  5,  2,  3,  2,  2,  3,  3,  2,  3,  2,  2,\n",
       "        1,  2,  1,  1,  3,  5,  1,  1,  1,  2,  2,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 10,  6,  3,  1,  2,  4,  2,  3,  2,  2,  3,  3,  2,  3,  2,  2,\n",
       "        1,  2,  1,  1,  3,  5,  1,  1,  1,  2,  2,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = (a.sum(0)!= b.sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True False False False False  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False]\n"
     ]
    }
   ],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = i[:,np.newaxis]*i[np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.triu(ms).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dot(np.triu(ms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_a = a*np.triu(ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_a.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False, False, False],\n",
       "       [False,  True, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       ...,\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = (a.sum(0)!= b.sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i[:, np.newaxis].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incidence(a):\n",
    "    \"\"\"make incidence matrix from adjacency matrix\"\"\"\n",
    "    ix = np.where(np.triu(a))\n",
    "    edg = array(list(zip(*ix)))\n",
    "    incid = np.zeros((len(a), len(edg)), dtype=np.float32)\n",
    "    for _,[i,j] in enumerate(edg): \n",
    "        incid[i,_] = 1\n",
    "        incid[j,_] = -1\n",
    "    return incid\n",
    "\n",
    "def incidence_shuffle(incid):\n",
    "    neg = np.where(incid < 0)\n",
    "    # preserve the rows, to preserve node degree, but shuffle columns, to shuffle links\n",
    "    np.random.shuffle(neg[1])\n",
    "    incid2 = 1.* (incid> 0 )\n",
    "    incid2[neg] -= 1\n",
    "    return incid2\n",
    "\n",
    "def configuration_model_1(a, weighted = True):\n",
    "    incid = get_incidence(a)\n",
    "    incid2 = incidence_shuffle(incid)\n",
    "    lap = incid2.dot(incid2.T)\n",
    "    deg = lap.diagonal() \n",
    "    a2 = np.diag(deg)-lap\n",
    "    if not weighted:\n",
    "        a2 = (1*(a2>0))\n",
    "    return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = configuration_model_1(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3., 10.,  6.,  3.,  1.,  2.,  5.,  2.,  3.,  2.,  2.,  3.,  3.,\n",
       "        2.,  3.,  2.,  2.,  1.,  2.,  1.,  1.,  3.,  5.,  1.,  1.,  1.,\n",
       "        2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 10,  6,  3,  1,  2,  4,  2,  3,  2,  2,  3,  3,  2,  3,  2,  2,\n",
       "        1,  2,  1,  1,  3,  5,  1,  1,  1,  2,  2,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 12,  6,  3,  1,  2,  5,  2,  3,  2,  2,  3,  3,  2,  3,  2,  2,\n",
       "        1,  2,  1,  1,  3,  5,  1,  1,  1,  2,  2,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum(0) != b.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = (a.sum(0) != d.sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.0"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[1][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-144-bbc30a54007c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrim_mat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-b09da91c8e92>\u001b[0m in \u001b[0;36mtrim_mat\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m!=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mmasked_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtriu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasked_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "trim_mat(a, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 6], dtype=int64), array([1, 6, 6], dtype=int64))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.triu(ms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0], dtype=int32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 12,  6,  3,  1,  2,  5,  2,  3,  2,  2,  3,  3,  2,  3,  2,  2,\n",
       "        1,  2,  1,  1,  3,  5,  1,  1,  1,  2,  2,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(graphdatabase.graphadjs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array(graphdatabase.graphadjs[2502])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 11,  5,  8, 10,  2,  3,  5,  1,  1,  2,  1,  7,  1,  1,  1,  1,\n",
       "        1,  2,  1,  2,  1,  2,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 9, 5, 6, 7, 2, 3, 4, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 2, 1, 2, 1,\n",
       "       2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = configuration_model_1(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., 11.,  5.,  8., 10.,  2.,  3.,  5.,  1.,  1.,  2.,  1.,  7.,\n",
       "        1.,  1.,  1.,  1.,  1.,  2.,  1.,  2.,  1.,  2.,  1.,  1.,  1.,\n",
       "        1.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hr21924\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:611: MatplotlibDeprecationWarning: isinstance(..., numbers.Number)\n",
      "  if cb.is_numlike(alpha):\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nx.draw_kamada_kawai(nx.Graph(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nx.draw_kamada_kawai(nx.Graph(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16,  4,  2,  7,  2,  2,  5,  3,  1,  1,  3,  3,  1,  1,  1,  3,  3,\n",
       "        1,  2,  1,  3,  1,  1,  2,  1,  1,  1,  1,  2,  1,  1,  1,  2,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  1,  1],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(array([], dtype=int64), array([], dtype=int64))\n",
      "0\n",
      "[]\n",
      "[16.  4.  2.  7.  2.  2.  5.  3.  1.  1.  3.  3.  1.  1.  1.  3.  3.  1.\n",
      "  2.  1.  3.  1.  1.  2.  1.  1.  1.  1.  2.  1.  1.  1.  2.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "i = (a.sum(0)!= b.sum(0))\n",
    "print(i)\n",
    "ms = i[:,np.newaxis]*(i[np.newaxis])\n",
    "masked_a = a*(np.triu(ms))\n",
    "print(masked_a.sum(0))\n",
    "ix = np.where(masked_a)\n",
    "print(ix)\n",
    "d = int(a.sum()-b.sum())//2\n",
    "print(d)\n",
    "r = np.argsort(np.random.rand(len(ix[0])))[:d]\n",
    "print(r)\n",
    "c = np.triu(a) + 0.\n",
    "for i in r:\n",
    "    c[ix[0][i],ix[1][i]]=0\n",
    "c += c.T \n",
    "print(c.sum(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_a.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50)\n"
     ]
    }
   ],
   "source": [
    "ms = i[:,np.newaxis]*(i[np.newaxis])\n",
    "print(ms.shape)\n",
    "masked_a = a*(np.triu(ms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = np.where(masked_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True False False False False  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False]\n",
      "(50, 50)\n",
      "(50, 50)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(array([], dtype=int64), array([], dtype=int64))\n",
      "1\n",
      "[]\n",
      "[ 3. 12.  6.  3.  1.  2.  5.  2.  3.  2.  2.  3.  3.  2.  3.  2.  2.  1.\n",
      "  2.  1.  1.  3.  5.  1.  1.  1.  2.  2.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "i = (a.sum(0)!= b.sum(0))\n",
    "print(i)\n",
    "ms = i[:,np.newaxis]*(i[np.newaxis])\n",
    "print(ms.shape)\n",
    "masked_a = a*(np.triu(ms))\n",
    "print(masked_a.shape)\n",
    "print(masked_a.sum(0))\n",
    "ix = np.where(masked_a)\n",
    "print(ix)\n",
    "d = int(a.sum()-b.sum())//2\n",
    "print(d)\n",
    "r = np.argsort(np.random.rand(len(ix[0])))[:d]\n",
    "print(r)\n",
    "c = np.triu(a) + 0.\n",
    "for i in r:\n",
    "    c[ix[0][i],ix[1][i]]=0\n",
    "c += c.T \n",
    "print(c.sum(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(a[i][j]) for i, j in zip(ix[0], ix[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(a)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "1\n",
      "0 6\n",
      "0\n",
      "2 1\n",
      "1\n",
      "2 6\n",
      "1\n",
      "4 1\n",
      "1\n",
      "4 6\n",
      "0\n",
      "5 1\n",
      "1\n",
      "5 6\n",
      "0\n",
      "8 1\n",
      "1\n",
      "8 6\n",
      "0\n",
      "9 6\n",
      "1\n",
      "12 6\n",
      "1\n",
      "15 1\n",
      "1\n",
      "15 6\n",
      "0\n",
      "20 1\n",
      "1\n",
      "20 6\n",
      "0\n",
      "21 1\n",
      "1\n",
      "21 6\n",
      "0\n",
      "24 6\n",
      "1\n",
      "38 1\n",
      "1\n",
      "38 6\n",
      "0\n",
      "40 1\n",
      "1\n",
      "40 6\n",
      "0\n",
      "41 1\n",
      "1\n",
      "41 6\n",
      "0\n",
      "42 6\n",
      "1\n",
      "48 1\n",
      "1\n",
      "48 6\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i, j in zip(ix[0], ix[1]):\n",
    "    print(i,j)\n",
    "    print(a[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.triu(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "c+= c.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 12,  6,  3,  1,  2,  5,  2,  3,  2,  2,  3,  3,  2,  3,  2,  2,\n",
       "        1,  2,  1,  1,  3,  5,  1,  1,  1,  2,  2,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 3, 12,  6,  3,  1,  2,  5,  2,  3,  2,  2,  3,  3,  2,  3,  2,\n",
       "          2,  1,  2,  1,  1,  3,  5,  1,  1,  1,  2,  2,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1]], dtype=int32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 3, 10,  6,  3,  1,  2,  4,  2,  3,  2,  2,  3,  3,  2,  3,  2,\n",
       "          2,  1,  2,  1,  1,  3,  5,  1,  1,  1,  2,  2,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1]], dtype=int32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.array(a.sum(0)!= b.sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True, False, False, False, False,  True, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = i[:,np.newaxis]*i[np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 50)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.triu(ms).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (50,50) and (1,50) not aligned: 50 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-4e24c521ce8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmasked_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtriu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\hr21924\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\numpy\\matrixlib\\defmatrix.py\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m             \u001b[1;31m# This promotes 1-D vectors to row vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0masmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__rmul__'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (50,50) and (1,50) not aligned: 50 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "masked_a = a*np.triu(ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Generate the map dataset for BA vs ER\n",
    "\n",
    "# map_dict = {\"BA\" : 1, \"ER\": 0}\n",
    "# n_classes = len(map_dict)\n",
    "\n",
    "# graphdatabase = GraphData(2500, 50)\n",
    "# graphdatabase.generate_BAGraphs()\n",
    "# graphdatabase.generate_ERGraphs()\n",
    "# graphdatabase.generate_adjs()\n",
    "# graphdatabase.normalise()\n",
    "# graphdatabase.tokenise(map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the length\n",
    "\n",
    "len(graphdatabase.graphlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(graphdatabase.graphadjs_dad)\n",
    "Y = np.expand_dims(np.array(graphdatabase.tokenisedgraphlabels), axis = 1)\n",
    "# Y = one_hot_encode(np.array(graphdatabase.tokenisedgraphlabels), n_classes)\n",
    "h = np.ones(X.shape[:2]+(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nnodes, graphoutputfeat):\n",
    "        super(GCN, self).__init__()\n",
    "        self.fc = nn.Linear(in_features=nnodes, out_features=graphoutputfeat)\n",
    "    def forward(self, input, adj):\n",
    "        x = F.relu(torch.bmm(adj, input))\n",
    "        x = x.view(-1, x.shape[-2])\n",
    "        x = F.softmax(self.fc(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 50, 50)\n",
      "(5000, 1)\n",
      "(5000, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = GCN(h.shape[-2], 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_a = graphdatabase.graphadjs[:2500].copy()\n",
    "g_b = graphdatabase.graphadjs[2500:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.MSELoss(reduction='mean')\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=.001)\n",
    "train_step = make_train_step(net, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hr21924\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n",
      "c:\\users\\hr21924\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy is: 0.5727500319480896\n",
      "[1] Training loss: 0.676\t Validation loss: 0.675\n",
      "Val Accuracy is: 0.5595000386238098\n",
      "[2] Training loss: 0.675\t Validation loss: 0.675\n",
      "Val Accuracy is: 0.7302500605583191\n",
      "[3] Training loss: 0.674\t Validation loss: 0.675\n",
      "Val Accuracy is: 0.7985000610351562\n",
      "[4] Training loss: 0.674\t Validation loss: 0.675\n",
      "Val Accuracy is: 0.7622500658035278\n",
      "[5] Training loss: 0.673\t Validation loss: 0.675\n",
      "Val Accuracy is: 0.7432500123977661\n",
      "[6] Training loss: 0.673\t Validation loss: 0.674\n",
      "Val Accuracy is: 0.7440000176429749\n",
      "[7] Training loss: 0.673\t Validation loss: 0.674\n",
      "Val Accuracy is: 0.7637500166893005\n",
      "[8] Training loss: 0.672\t Validation loss: 0.674\n",
      "Val Accuracy is: 0.7890000343322754\n",
      "[9] Training loss: 0.672\t Validation loss: 0.673\n",
      "Val Accuracy is: 0.811750054359436\n",
      "[10] Training loss: 0.671\t Validation loss: 0.672\n",
      "Val Accuracy is: 0.8290000557899475\n",
      "[11] Training loss: 0.671\t Validation loss: 0.672\n",
      "Val Accuracy is: 0.7355000376701355\n",
      "[12] Training loss: 0.670\t Validation loss: 0.671\n",
      "Val Accuracy is: 0.6425000429153442\n",
      "[13] Training loss: 0.670\t Validation loss: 0.671\n",
      "Val Accuracy is: 0.5492500066757202\n",
      "[14] Training loss: 0.669\t Validation loss: 0.671\n",
      "Val Accuracy is: 0.5515000224113464\n",
      "[15] Training loss: 0.669\t Validation loss: 0.670\n",
      "Val Accuracy is: 0.5520000457763672\n",
      "[16] Training loss: 0.668\t Validation loss: 0.670\n",
      "Val Accuracy is: 0.5525000095367432\n",
      "[17] Training loss: 0.668\t Validation loss: 0.669\n",
      "Val Accuracy is: 0.5522500276565552\n",
      "[18] Training loss: 0.668\t Validation loss: 0.669\n",
      "Val Accuracy is: 0.5517500042915344\n",
      "[19] Training loss: 0.667\t Validation loss: 0.668\n",
      "Val Accuracy is: 0.5515000224113464\n",
      "[20] Training loss: 0.667\t Validation loss: 0.668\n",
      "Val Accuracy is: 0.5512500405311584\n",
      "[21] Training loss: 0.666\t Validation loss: 0.668\n",
      "Val Accuracy is: 0.6455000042915344\n",
      "[22] Training loss: 0.666\t Validation loss: 0.667\n",
      "Val Accuracy is: 0.6447500586509705\n",
      "[23] Training loss: 0.665\t Validation loss: 0.667\n",
      "Val Accuracy is: 0.643250048160553\n",
      "[24] Training loss: 0.665\t Validation loss: 0.667\n",
      "Val Accuracy is: 0.7462500333786011\n",
      "[25] Training loss: 0.665\t Validation loss: 0.666\n",
      "Val Accuracy is: 0.7455000281333923\n",
      "[26] Training loss: 0.664\t Validation loss: 0.666\n",
      "Val Accuracy is: 0.7450000643730164\n",
      "[27] Training loss: 0.664\t Validation loss: 0.666\n",
      "Val Accuracy is: 0.7452500462532043\n",
      "[28] Training loss: 0.663\t Validation loss: 0.665\n",
      "Val Accuracy is: 0.7452500462532043\n",
      "[29] Training loss: 0.663\t Validation loss: 0.665\n",
      "Val Accuracy is: 0.7467500567436218\n",
      "[30] Training loss: 0.663\t Validation loss: 0.664\n",
      "Val Accuracy is: 0.7467500567436218\n",
      "[31] Training loss: 0.662\t Validation loss: 0.664\n",
      "Val Accuracy is: 0.7472500205039978\n",
      "[32] Training loss: 0.662\t Validation loss: 0.664\n",
      "Val Accuracy is: 0.7485000491142273\n",
      "[33] Training loss: 0.661\t Validation loss: 0.663\n",
      "Val Accuracy is: 0.7490000128746033\n",
      "[34] Training loss: 0.661\t Validation loss: 0.663\n",
      "Val Accuracy is: 0.749250054359436\n",
      "[35] Training loss: 0.660\t Validation loss: 0.662\n",
      "Val Accuracy is: 0.749750018119812\n",
      "[36] Training loss: 0.660\t Validation loss: 0.662\n",
      "Val Accuracy is: 0.7505000233650208\n",
      "[37] Training loss: 0.660\t Validation loss: 0.662\n",
      "Val Accuracy is: 0.7512500286102295\n",
      "[38] Training loss: 0.659\t Validation loss: 0.661\n",
      "Val Accuracy is: 0.7515000104904175\n",
      "[39] Training loss: 0.659\t Validation loss: 0.661\n",
      "Val Accuracy is: 0.7517500519752502\n",
      "[40] Training loss: 0.658\t Validation loss: 0.660\n",
      "Val Accuracy is: 0.7515000104904175\n",
      "[41] Training loss: 0.658\t Validation loss: 0.660\n",
      "Val Accuracy is: 0.7515000104904175\n",
      "[42] Training loss: 0.658\t Validation loss: 0.660\n",
      "Val Accuracy is: 0.7515000104904175\n",
      "[43] Training loss: 0.657\t Validation loss: 0.659\n",
      "Val Accuracy is: 0.7517500519752502\n",
      "[44] Training loss: 0.657\t Validation loss: 0.659\n",
      "Val Accuracy is: 0.7517500519752502\n",
      "[45] Training loss: 0.656\t Validation loss: 0.659\n",
      "Val Accuracy is: 0.7517500519752502\n",
      "[46] Training loss: 0.656\t Validation loss: 0.658\n",
      "Val Accuracy is: 0.7520000338554382\n",
      "[47] Training loss: 0.656\t Validation loss: 0.658\n",
      "Val Accuracy is: 0.752500057220459\n",
      "[48] Training loss: 0.655\t Validation loss: 0.658\n",
      "Val Accuracy is: 0.752500057220459\n",
      "[49] Training loss: 0.655\t Validation loss: 0.657\n",
      "Val Accuracy is: 0.7535000443458557\n",
      "[50] Training loss: 0.654\t Validation loss: 0.657\n",
      "Val Accuracy is: 0.7542500495910645\n",
      "[51] Training loss: 0.654\t Validation loss: 0.656\n",
      "Val Accuracy is: 0.7550000548362732\n",
      "[52] Training loss: 0.654\t Validation loss: 0.656\n",
      "Val Accuracy is: 0.7555000185966492\n",
      "[53] Training loss: 0.653\t Validation loss: 0.656\n",
      "Val Accuracy is: 0.7557500600814819\n",
      "[54] Training loss: 0.653\t Validation loss: 0.655\n",
      "Val Accuracy is: 0.7565000653266907\n",
      "[55] Training loss: 0.652\t Validation loss: 0.655\n",
      "Val Accuracy is: 0.7575000524520874\n",
      "[56] Training loss: 0.652\t Validation loss: 0.655\n",
      "Val Accuracy is: 0.7582500576972961\n",
      "[57] Training loss: 0.652\t Validation loss: 0.654\n",
      "Val Accuracy is: 0.7582500576972961\n",
      "[58] Training loss: 0.651\t Validation loss: 0.654\n",
      "Val Accuracy is: 0.7582500576972961\n",
      "[59] Training loss: 0.651\t Validation loss: 0.653\n",
      "Val Accuracy is: 0.7587500214576721\n",
      "[60] Training loss: 0.651\t Validation loss: 0.653\n",
      "Val Accuracy is: 0.7585000395774841\n",
      "[61] Training loss: 0.650\t Validation loss: 0.653\n",
      "Val Accuracy is: 0.7590000629425049\n",
      "[62] Training loss: 0.650\t Validation loss: 0.652\n",
      "Val Accuracy is: 0.7595000267028809\n",
      "[63] Training loss: 0.649\t Validation loss: 0.652\n",
      "Val Accuracy is: 0.7597500085830688\n",
      "[64] Training loss: 0.649\t Validation loss: 0.652\n",
      "Val Accuracy is: 0.7600000500679016\n",
      "[65] Training loss: 0.649\t Validation loss: 0.651\n",
      "Val Accuracy is: 0.7602500319480896\n",
      "[66] Training loss: 0.648\t Validation loss: 0.651\n",
      "Val Accuracy is: 0.7610000371932983\n",
      "[67] Training loss: 0.648\t Validation loss: 0.651\n",
      "Val Accuracy is: 0.7605000138282776\n",
      "[68] Training loss: 0.648\t Validation loss: 0.650\n",
      "Val Accuracy is: 0.7607500553131104\n",
      "[69] Training loss: 0.647\t Validation loss: 0.650\n",
      "Val Accuracy is: 0.7612500190734863\n",
      "[70] Training loss: 0.647\t Validation loss: 0.650\n",
      "Val Accuracy is: 0.7615000605583191\n",
      "[71] Training loss: 0.646\t Validation loss: 0.649\n",
      "Val Accuracy is: 0.7622500658035278\n",
      "[72] Training loss: 0.646\t Validation loss: 0.649\n",
      "Val Accuracy is: 0.7622500658035278\n",
      "[73] Training loss: 0.646\t Validation loss: 0.649\n",
      "Val Accuracy is: 0.7625000476837158\n",
      "[74] Training loss: 0.645\t Validation loss: 0.648\n",
      "Val Accuracy is: 0.7632500529289246\n",
      "[75] Training loss: 0.645\t Validation loss: 0.648\n",
      "Val Accuracy is: 0.7635000348091125\n",
      "[76] Training loss: 0.645\t Validation loss: 0.647\n",
      "Val Accuracy is: 0.7642500400543213\n",
      "[77] Training loss: 0.644\t Validation loss: 0.647\n",
      "Val Accuracy is: 0.7640000581741333\n",
      "[78] Training loss: 0.644\t Validation loss: 0.647\n",
      "Val Accuracy is: 0.7642500400543213\n",
      "[79] Training loss: 0.643\t Validation loss: 0.646\n",
      "Val Accuracy is: 0.7645000219345093\n",
      "[80] Training loss: 0.643\t Validation loss: 0.646\n",
      "Val Accuracy is: 0.7645000219345093\n",
      "[81] Training loss: 0.643\t Validation loss: 0.646\n",
      "Val Accuracy is: 0.7640000581741333\n",
      "[82] Training loss: 0.642\t Validation loss: 0.645\n",
      "Val Accuracy is: 0.7642500400543213\n",
      "[83] Training loss: 0.642\t Validation loss: 0.645\n",
      "Val Accuracy is: 0.7642500400543213\n",
      "[84] Training loss: 0.642\t Validation loss: 0.645\n",
      "Val Accuracy is: 0.764750063419342\n",
      "[85] Training loss: 0.641\t Validation loss: 0.644\n",
      "Val Accuracy is: 0.76500004529953\n",
      "[86] Training loss: 0.641\t Validation loss: 0.644\n",
      "Val Accuracy is: 0.76500004529953\n",
      "[87] Training loss: 0.641\t Validation loss: 0.644\n",
      "Val Accuracy is: 0.764750063419342\n",
      "[88] Training loss: 0.640\t Validation loss: 0.643\n",
      "Val Accuracy is: 0.7645000219345093\n",
      "[89] Training loss: 0.640\t Validation loss: 0.643\n",
      "Val Accuracy is: 0.764750063419342\n",
      "[90] Training loss: 0.639\t Validation loss: 0.643\n",
      "Val Accuracy is: 0.764750063419342\n",
      "[91] Training loss: 0.639\t Validation loss: 0.642\n",
      "Val Accuracy is: 0.764750063419342\n",
      "[92] Training loss: 0.639\t Validation loss: 0.642\n",
      "Val Accuracy is: 0.7645000219345093\n",
      "[93] Training loss: 0.638\t Validation loss: 0.642\n",
      "Val Accuracy is: 0.7645000219345093\n",
      "[94] Training loss: 0.638\t Validation loss: 0.641\n",
      "Val Accuracy is: 0.7642500400543213\n",
      "[95] Training loss: 0.638\t Validation loss: 0.641\n",
      "Val Accuracy is: 0.7642500400543213\n",
      "[96] Training loss: 0.637\t Validation loss: 0.641\n",
      "Val Accuracy is: 0.7640000581741333\n",
      "[97] Training loss: 0.637\t Validation loss: 0.640\n",
      "Val Accuracy is: 0.7642500400543213\n",
      "[98] Training loss: 0.637\t Validation loss: 0.640\n",
      "Val Accuracy is: 0.76500004529953\n",
      "[99] Training loss: 0.636\t Validation loss: 0.640\n",
      "Val Accuracy is: 0.76500004529953\n",
      "[100] Training loss: 0.636\t Validation loss: 0.639\n",
      "Val Accuracy is: 0.764750063419342\n",
      "[101] Training loss: 0.636\t Validation loss: 0.639\n",
      "Val Accuracy is: 0.7645000219345093\n",
      "[102] Training loss: 0.635\t Validation loss: 0.639\n",
      "Val Accuracy is: 0.7645000219345093\n",
      "[103] Training loss: 0.635\t Validation loss: 0.638\n",
      "Val Accuracy is: 0.7645000219345093\n",
      "[104] Training loss: 0.635\t Validation loss: 0.638\n",
      "Val Accuracy is: 0.7645000219345093\n",
      "[105] Training loss: 0.634\t Validation loss: 0.638\n",
      "Val Accuracy is: 0.7645000219345093\n",
      "[106] Training loss: 0.634\t Validation loss: 0.637\n",
      "Val Accuracy is: 0.765250027179718\n",
      "[107] Training loss: 0.634\t Validation loss: 0.637\n",
      "Val Accuracy is: 0.7657500505447388\n",
      "[108] Training loss: 0.633\t Validation loss: 0.637\n",
      "Val Accuracy is: 0.7657500505447388\n",
      "[109] Training loss: 0.633\t Validation loss: 0.637\n",
      "Val Accuracy is: 0.7660000324249268\n",
      "[110] Training loss: 0.633\t Validation loss: 0.636\n",
      "Val Accuracy is: 0.7660000324249268\n",
      "[111] Training loss: 0.632\t Validation loss: 0.636\n",
      "Val Accuracy is: 0.765500009059906\n",
      "[112] Training loss: 0.632\t Validation loss: 0.636\n",
      "Val Accuracy is: 0.765250027179718\n",
      "[113] Training loss: 0.632\t Validation loss: 0.635\n",
      "Val Accuracy is: 0.765250027179718\n",
      "[114] Training loss: 0.631\t Validation loss: 0.635\n",
      "Val Accuracy is: 0.765250027179718\n",
      "[115] Training loss: 0.631\t Validation loss: 0.635\n",
      "Val Accuracy is: 0.765250027179718\n",
      "[116] Training loss: 0.631\t Validation loss: 0.634\n",
      "Val Accuracy is: 0.76500004529953\n",
      "[117] Training loss: 0.630\t Validation loss: 0.634\n",
      "Val Accuracy is: 0.765250027179718\n",
      "[118] Training loss: 0.630\t Validation loss: 0.634\n",
      "Val Accuracy is: 0.7657500505447388\n",
      "[119] Training loss: 0.630\t Validation loss: 0.633\n",
      "Val Accuracy is: 0.765250027179718\n",
      "[120] Training loss: 0.629\t Validation loss: 0.633\n",
      "Val Accuracy is: 0.765250027179718\n",
      "[121] Training loss: 0.629\t Validation loss: 0.633\n",
      "Val Accuracy is: 0.765500009059906\n",
      "[122] Training loss: 0.629\t Validation loss: 0.632\n",
      "Val Accuracy is: 0.765500009059906\n",
      "[123] Training loss: 0.628\t Validation loss: 0.632\n",
      "Val Accuracy is: 0.765500009059906\n",
      "[124] Training loss: 0.628\t Validation loss: 0.632\n",
      "Val Accuracy is: 0.765500009059906\n",
      "[125] Training loss: 0.628\t Validation loss: 0.631\n",
      "Val Accuracy is: 0.765500009059906\n",
      "[126] Training loss: 0.627\t Validation loss: 0.631\n",
      "Val Accuracy is: 0.765250027179718\n",
      "[127] Training loss: 0.627\t Validation loss: 0.631\n",
      "Val Accuracy is: 0.765250027179718\n",
      "[128] Training loss: 0.627\t Validation loss: 0.631\n",
      "Val Accuracy is: 0.765250027179718\n",
      "[129] Training loss: 0.626\t Validation loss: 0.630\n",
      "Val Accuracy is: 0.765500009059906\n",
      "[130] Training loss: 0.626\t Validation loss: 0.630\n",
      "Val Accuracy is: 0.765250027179718\n",
      "[131] Training loss: 0.626\t Validation loss: 0.630\n",
      "Val Accuracy is: 0.765250027179718\n",
      "[132] Training loss: 0.625\t Validation loss: 0.629\n",
      "Val Accuracy is: 0.765250027179718\n",
      "[133] Training loss: 0.625\t Validation loss: 0.629\n",
      "Val Accuracy is: 0.76500004529953\n",
      "[134] Training loss: 0.625\t Validation loss: 0.629\n",
      "Val Accuracy is: 0.765500009059906\n",
      "[135] Training loss: 0.624\t Validation loss: 0.628\n",
      "Val Accuracy is: 0.765500009059906\n",
      "[136] Training loss: 0.624\t Validation loss: 0.628\n",
      "Val Accuracy is: 0.765250027179718\n",
      "[137] Training loss: 0.624\t Validation loss: 0.628\n",
      "Val Accuracy is: 0.764750063419342\n",
      "[138] Training loss: 0.623\t Validation loss: 0.627\n",
      "Val Accuracy is: 0.764750063419342\n",
      "[139] Training loss: 0.623\t Validation loss: 0.627\n",
      "Val Accuracy is: 0.76500004529953\n",
      "[140] Training loss: 0.623\t Validation loss: 0.627\n",
      "Val Accuracy is: 0.765250027179718\n",
      "[141] Training loss: 0.622\t Validation loss: 0.627\n",
      "Val Accuracy is: 0.765250027179718\n",
      "[142] Training loss: 0.622\t Validation loss: 0.626\n",
      "Val Accuracy is: 0.765500009059906\n",
      "[143] Training loss: 0.622\t Validation loss: 0.626\n",
      "Val Accuracy is: 0.765500009059906\n",
      "[144] Training loss: 0.622\t Validation loss: 0.626\n",
      "Val Accuracy is: 0.7657500505447388\n",
      "[145] Training loss: 0.621\t Validation loss: 0.625\n",
      "Val Accuracy is: 0.7657500505447388\n",
      "[146] Training loss: 0.621\t Validation loss: 0.625\n",
      "Val Accuracy is: 0.7660000324249268\n",
      "[147] Training loss: 0.621\t Validation loss: 0.625\n",
      "Val Accuracy is: 0.7660000324249268\n",
      "[148] Training loss: 0.620\t Validation loss: 0.624\n",
      "Val Accuracy is: 0.7660000324249268\n",
      "[149] Training loss: 0.620\t Validation loss: 0.624\n",
      "Val Accuracy is: 0.7660000324249268\n",
      "[150] Training loss: 0.620\t Validation loss: 0.624\n",
      "Val Accuracy is: 0.7660000324249268\n",
      "[151] Training loss: 0.619\t Validation loss: 0.624\n",
      "Val Accuracy is: 0.7662500143051147\n",
      "[152] Training loss: 0.619\t Validation loss: 0.623\n",
      "Val Accuracy is: 0.7665000557899475\n",
      "[153] Training loss: 0.619\t Validation loss: 0.623\n",
      "Val Accuracy is: 0.7665000557899475\n",
      "[154] Training loss: 0.618\t Validation loss: 0.623\n",
      "Val Accuracy is: 0.7665000557899475\n",
      "[155] Training loss: 0.618\t Validation loss: 0.622\n",
      "Val Accuracy is: 0.7667500376701355\n",
      "[156] Training loss: 0.618\t Validation loss: 0.622\n",
      "Val Accuracy is: 0.7662500143051147\n",
      "[157] Training loss: 0.618\t Validation loss: 0.622\n",
      "Val Accuracy is: 0.7662500143051147\n",
      "[158] Training loss: 0.617\t Validation loss: 0.622\n",
      "Val Accuracy is: 0.7662500143051147\n",
      "[159] Training loss: 0.617\t Validation loss: 0.621\n",
      "Val Accuracy is: 0.7660000324249268\n",
      "[160] Training loss: 0.617\t Validation loss: 0.621\n",
      "Val Accuracy is: 0.7660000324249268\n",
      "[161] Training loss: 0.616\t Validation loss: 0.621\n",
      "Val Accuracy is: 0.7660000324249268\n",
      "[162] Training loss: 0.616\t Validation loss: 0.620\n",
      "Val Accuracy is: 0.7662500143051147\n",
      "[163] Training loss: 0.616\t Validation loss: 0.620\n",
      "Val Accuracy is: 0.7662500143051147\n",
      "[164] Training loss: 0.615\t Validation loss: 0.620\n",
      "Val Accuracy is: 0.7665000557899475\n",
      "[165] Training loss: 0.615\t Validation loss: 0.619\n",
      "Val Accuracy is: 0.7665000557899475\n",
      "[166] Training loss: 0.615\t Validation loss: 0.619\n",
      "Val Accuracy is: 0.7665000557899475\n",
      "[167] Training loss: 0.614\t Validation loss: 0.619\n",
      "Val Accuracy is: 0.7665000557899475\n",
      "[168] Training loss: 0.614\t Validation loss: 0.619\n",
      "Val Accuracy is: 0.7665000557899475\n",
      "[169] Training loss: 0.614\t Validation loss: 0.618\n",
      "Val Accuracy is: 0.7667500376701355\n",
      "[170] Training loss: 0.614\t Validation loss: 0.618\n",
      "Val Accuracy is: 0.7667500376701355\n",
      "[171] Training loss: 0.613\t Validation loss: 0.618\n",
      "Val Accuracy is: 0.7667500376701355\n",
      "[172] Training loss: 0.613\t Validation loss: 0.617\n",
      "Val Accuracy is: 0.7667500376701355\n",
      "[173] Training loss: 0.613\t Validation loss: 0.617\n",
      "Val Accuracy is: 0.7670000195503235\n",
      "[174] Training loss: 0.612\t Validation loss: 0.617\n",
      "Val Accuracy is: 0.7670000195503235\n",
      "[175] Training loss: 0.612\t Validation loss: 0.617\n",
      "Val Accuracy is: 0.7670000195503235\n",
      "[176] Training loss: 0.612\t Validation loss: 0.616\n",
      "Val Accuracy is: 0.7670000195503235\n",
      "[177] Training loss: 0.612\t Validation loss: 0.616\n",
      "Val Accuracy is: 0.7670000195503235\n",
      "[178] Training loss: 0.611\t Validation loss: 0.616\n",
      "Val Accuracy is: 0.7670000195503235\n",
      "[179] Training loss: 0.611\t Validation loss: 0.615\n",
      "Val Accuracy is: 0.7670000195503235\n",
      "[180] Training loss: 0.611\t Validation loss: 0.615\n",
      "Val Accuracy is: 0.7670000195503235\n",
      "[181] Training loss: 0.610\t Validation loss: 0.615\n",
      "Val Accuracy is: 0.7670000195503235\n",
      "[182] Training loss: 0.610\t Validation loss: 0.615\n",
      "Val Accuracy is: 0.7670000195503235\n",
      "[183] Training loss: 0.610\t Validation loss: 0.614\n",
      "Val Accuracy is: 0.7670000195503235\n",
      "[184] Training loss: 0.609\t Validation loss: 0.614\n",
      "Val Accuracy is: 0.7670000195503235\n",
      "[185] Training loss: 0.609\t Validation loss: 0.614\n",
      "Val Accuracy is: 0.7672500610351562\n",
      "[186] Training loss: 0.609\t Validation loss: 0.614\n",
      "Val Accuracy is: 0.7672500610351562\n",
      "[187] Training loss: 0.609\t Validation loss: 0.613\n",
      "Val Accuracy is: 0.7675000429153442\n",
      "[188] Training loss: 0.608\t Validation loss: 0.613\n",
      "Val Accuracy is: 0.7672500610351562\n",
      "[189] Training loss: 0.608\t Validation loss: 0.613\n",
      "Val Accuracy is: 0.7672500610351562\n",
      "[190] Training loss: 0.608\t Validation loss: 0.612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy is: 0.7672500610351562\n",
      "[191] Training loss: 0.607\t Validation loss: 0.612\n",
      "Val Accuracy is: 0.7677500247955322\n",
      "[192] Training loss: 0.607\t Validation loss: 0.612\n",
      "Val Accuracy is: 0.7677500247955322\n",
      "[193] Training loss: 0.607\t Validation loss: 0.612\n",
      "Val Accuracy is: 0.7675000429153442\n",
      "[194] Training loss: 0.607\t Validation loss: 0.611\n",
      "Val Accuracy is: 0.7677500247955322\n",
      "[195] Training loss: 0.606\t Validation loss: 0.611\n",
      "Val Accuracy is: 0.7680000066757202\n",
      "[196] Training loss: 0.606\t Validation loss: 0.611\n",
      "Val Accuracy is: 0.768250048160553\n",
      "[197] Training loss: 0.606\t Validation loss: 0.610\n",
      "Val Accuracy is: 0.768250048160553\n",
      "[198] Training loss: 0.605\t Validation loss: 0.610\n",
      "Val Accuracy is: 0.768250048160553\n",
      "[199] Training loss: 0.605\t Validation loss: 0.610\n",
      "Val Accuracy is: 0.7680000066757202\n",
      "[200] Training loss: 0.605\t Validation loss: 0.610\n",
      "Val Accuracy is: 0.768250048160553\n",
      "[201] Training loss: 0.605\t Validation loss: 0.609\n",
      "Val Accuracy is: 0.768250048160553\n",
      "[202] Training loss: 0.604\t Validation loss: 0.609\n",
      "Val Accuracy is: 0.768250048160553\n",
      "[203] Training loss: 0.604\t Validation loss: 0.609\n",
      "Val Accuracy is: 0.7680000066757202\n",
      "[204] Training loss: 0.604\t Validation loss: 0.609\n",
      "Val Accuracy is: 0.768250048160553\n",
      "[205] Training loss: 0.603\t Validation loss: 0.608\n",
      "Val Accuracy is: 0.768250048160553\n",
      "[206] Training loss: 0.603\t Validation loss: 0.608\n",
      "Val Accuracy is: 0.7680000066757202\n",
      "[207] Training loss: 0.603\t Validation loss: 0.608\n",
      "Val Accuracy is: 0.7680000066757202\n",
      "[208] Training loss: 0.603\t Validation loss: 0.607\n",
      "Val Accuracy is: 0.768250048160553\n",
      "[209] Training loss: 0.602\t Validation loss: 0.607\n",
      "Val Accuracy is: 0.768250048160553\n",
      "[210] Training loss: 0.602\t Validation loss: 0.607\n",
      "Val Accuracy is: 0.8680000305175781\n",
      "[211] Training loss: 0.602\t Validation loss: 0.607\n",
      "Val Accuracy is: 0.8677500486373901\n",
      "[212] Training loss: 0.602\t Validation loss: 0.606\n",
      "Val Accuracy is: 0.8677500486373901\n",
      "[213] Training loss: 0.601\t Validation loss: 0.606\n",
      "Val Accuracy is: 0.8677500486373901\n",
      "[214] Training loss: 0.601\t Validation loss: 0.606\n",
      "Val Accuracy is: 0.8677500486373901\n",
      "[215] Training loss: 0.601\t Validation loss: 0.606\n",
      "Val Accuracy is: 0.8680000305175781\n",
      "[216] Training loss: 0.600\t Validation loss: 0.605\n",
      "Val Accuracy is: 0.8680000305175781\n",
      "[217] Training loss: 0.600\t Validation loss: 0.605\n",
      "Val Accuracy is: 0.8680000305175781\n",
      "[218] Training loss: 0.600\t Validation loss: 0.605\n",
      "Val Accuracy is: 0.8682500123977661\n",
      "[219] Training loss: 0.600\t Validation loss: 0.604\n",
      "Val Accuracy is: 0.8682500123977661\n",
      "[220] Training loss: 0.599\t Validation loss: 0.604\n",
      "Val Accuracy is: 0.8682500123977661\n",
      "[221] Training loss: 0.599\t Validation loss: 0.604\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[222] Training loss: 0.599\t Validation loss: 0.604\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[223] Training loss: 0.599\t Validation loss: 0.603\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[224] Training loss: 0.598\t Validation loss: 0.603\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[225] Training loss: 0.598\t Validation loss: 0.603\n",
      "Val Accuracy is: 0.8680000305175781\n",
      "[226] Training loss: 0.598\t Validation loss: 0.603\n",
      "Val Accuracy is: 0.8680000305175781\n",
      "[227] Training loss: 0.597\t Validation loss: 0.602\n",
      "Val Accuracy is: 0.8680000305175781\n",
      "[228] Training loss: 0.597\t Validation loss: 0.602\n",
      "Val Accuracy is: 0.8680000305175781\n",
      "[229] Training loss: 0.597\t Validation loss: 0.602\n",
      "Val Accuracy is: 0.8682500123977661\n",
      "[230] Training loss: 0.597\t Validation loss: 0.602\n",
      "Val Accuracy is: 0.8682500123977661\n",
      "[231] Training loss: 0.596\t Validation loss: 0.601\n",
      "Val Accuracy is: 0.8682500123977661\n",
      "[232] Training loss: 0.596\t Validation loss: 0.601\n",
      "Val Accuracy is: 0.8682500123977661\n",
      "[233] Training loss: 0.596\t Validation loss: 0.601\n",
      "Val Accuracy is: 0.8682500123977661\n",
      "[234] Training loss: 0.596\t Validation loss: 0.601\n",
      "Val Accuracy is: 0.8682500123977661\n",
      "[235] Training loss: 0.595\t Validation loss: 0.600\n",
      "Val Accuracy is: 0.8682500123977661\n",
      "[236] Training loss: 0.595\t Validation loss: 0.600\n",
      "Val Accuracy is: 0.8682500123977661\n",
      "[237] Training loss: 0.595\t Validation loss: 0.600\n",
      "Val Accuracy is: 0.8680000305175781\n",
      "[238] Training loss: 0.595\t Validation loss: 0.599\n",
      "Val Accuracy is: 0.8680000305175781\n",
      "[239] Training loss: 0.594\t Validation loss: 0.599\n",
      "Val Accuracy is: 0.8680000305175781\n",
      "[240] Training loss: 0.594\t Validation loss: 0.599\n",
      "Val Accuracy is: 0.8682500123977661\n",
      "[241] Training loss: 0.594\t Validation loss: 0.599\n",
      "Val Accuracy is: 0.8682500123977661\n",
      "[242] Training loss: 0.593\t Validation loss: 0.598\n",
      "Val Accuracy is: 0.8682500123977661\n",
      "[243] Training loss: 0.593\t Validation loss: 0.598\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[244] Training loss: 0.593\t Validation loss: 0.598\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[245] Training loss: 0.593\t Validation loss: 0.598\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[246] Training loss: 0.592\t Validation loss: 0.597\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[247] Training loss: 0.592\t Validation loss: 0.597\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[248] Training loss: 0.592\t Validation loss: 0.597\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[249] Training loss: 0.592\t Validation loss: 0.597\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[250] Training loss: 0.591\t Validation loss: 0.596\n",
      "Val Accuracy is: 0.8690000176429749\n",
      "[251] Training loss: 0.591\t Validation loss: 0.596\n",
      "Val Accuracy is: 0.8690000176429749\n",
      "[252] Training loss: 0.591\t Validation loss: 0.596\n",
      "Val Accuracy is: 0.8690000176429749\n",
      "[253] Training loss: 0.591\t Validation loss: 0.596\n",
      "Val Accuracy is: 0.8690000176429749\n",
      "[254] Training loss: 0.590\t Validation loss: 0.595\n",
      "Val Accuracy is: 0.8695000410079956\n",
      "[255] Training loss: 0.590\t Validation loss: 0.595\n",
      "Val Accuracy is: 0.8695000410079956\n",
      "[256] Training loss: 0.590\t Validation loss: 0.595\n",
      "Val Accuracy is: 0.8695000410079956\n",
      "[257] Training loss: 0.590\t Validation loss: 0.595\n",
      "Val Accuracy is: 0.8692500591278076\n",
      "[258] Training loss: 0.589\t Validation loss: 0.594\n",
      "Val Accuracy is: 0.8692500591278076\n",
      "[259] Training loss: 0.589\t Validation loss: 0.594\n",
      "Val Accuracy is: 0.8695000410079956\n",
      "[260] Training loss: 0.589\t Validation loss: 0.594\n",
      "Val Accuracy is: 0.8695000410079956\n",
      "[261] Training loss: 0.589\t Validation loss: 0.594\n",
      "Val Accuracy is: 0.8695000410079956\n",
      "[262] Training loss: 0.588\t Validation loss: 0.593\n",
      "Val Accuracy is: 0.8695000410079956\n",
      "[263] Training loss: 0.588\t Validation loss: 0.593\n",
      "Val Accuracy is: 0.8695000410079956\n",
      "[264] Training loss: 0.588\t Validation loss: 0.593\n",
      "Val Accuracy is: 0.8690000176429749\n",
      "[265] Training loss: 0.588\t Validation loss: 0.593\n",
      "Val Accuracy is: 0.8690000176429749\n",
      "[266] Training loss: 0.587\t Validation loss: 0.592\n",
      "Val Accuracy is: 0.8690000176429749\n",
      "[267] Training loss: 0.587\t Validation loss: 0.592\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[268] Training loss: 0.587\t Validation loss: 0.592\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[269] Training loss: 0.587\t Validation loss: 0.592\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[270] Training loss: 0.586\t Validation loss: 0.591\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[271] Training loss: 0.586\t Validation loss: 0.591\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[272] Training loss: 0.586\t Validation loss: 0.591\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[273] Training loss: 0.586\t Validation loss: 0.591\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[274] Training loss: 0.585\t Validation loss: 0.590\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[275] Training loss: 0.585\t Validation loss: 0.590\n",
      "Val Accuracy is: 0.8690000176429749\n",
      "[276] Training loss: 0.585\t Validation loss: 0.590\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[277] Training loss: 0.585\t Validation loss: 0.590\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[278] Training loss: 0.584\t Validation loss: 0.589\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[279] Training loss: 0.584\t Validation loss: 0.589\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[280] Training loss: 0.584\t Validation loss: 0.589\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[281] Training loss: 0.584\t Validation loss: 0.589\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[282] Training loss: 0.583\t Validation loss: 0.588\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[283] Training loss: 0.583\t Validation loss: 0.588\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[284] Training loss: 0.583\t Validation loss: 0.588\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[285] Training loss: 0.583\t Validation loss: 0.588\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[286] Training loss: 0.582\t Validation loss: 0.587\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[287] Training loss: 0.582\t Validation loss: 0.587\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[288] Training loss: 0.582\t Validation loss: 0.587\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[289] Training loss: 0.582\t Validation loss: 0.587\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[290] Training loss: 0.581\t Validation loss: 0.587\n",
      "Val Accuracy is: 0.8690000176429749\n",
      "[291] Training loss: 0.581\t Validation loss: 0.586\n",
      "Val Accuracy is: 0.8690000176429749\n",
      "[292] Training loss: 0.581\t Validation loss: 0.586\n",
      "Val Accuracy is: 0.8690000176429749\n",
      "[293] Training loss: 0.581\t Validation loss: 0.586\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[294] Training loss: 0.580\t Validation loss: 0.586\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[295] Training loss: 0.580\t Validation loss: 0.585\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[296] Training loss: 0.580\t Validation loss: 0.585\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[297] Training loss: 0.580\t Validation loss: 0.585\n",
      "Val Accuracy is: 0.8690000176429749\n",
      "[298] Training loss: 0.579\t Validation loss: 0.585\n",
      "Val Accuracy is: 0.8687500357627869\n",
      "[299] Training loss: 0.579\t Validation loss: 0.584\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[300] Training loss: 0.579\t Validation loss: 0.584\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[301] Training loss: 0.579\t Validation loss: 0.584\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[302] Training loss: 0.579\t Validation loss: 0.584\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[303] Training loss: 0.578\t Validation loss: 0.583\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[304] Training loss: 0.578\t Validation loss: 0.583\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[305] Training loss: 0.578\t Validation loss: 0.583\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[306] Training loss: 0.578\t Validation loss: 0.583\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[307] Training loss: 0.577\t Validation loss: 0.583\n",
      "Val Accuracy is: 0.8682500123977661\n",
      "[308] Training loss: 0.577\t Validation loss: 0.582\n",
      "Val Accuracy is: 0.8682500123977661\n",
      "[309] Training loss: 0.577\t Validation loss: 0.582\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[310] Training loss: 0.577\t Validation loss: 0.582\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[311] Training loss: 0.576\t Validation loss: 0.582\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[312] Training loss: 0.576\t Validation loss: 0.581\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[313] Training loss: 0.576\t Validation loss: 0.581\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[314] Training loss: 0.576\t Validation loss: 0.581\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[315] Training loss: 0.575\t Validation loss: 0.581\n",
      "Val Accuracy is: 0.8685000538825989\n",
      "[316] Training loss: 0.575\t Validation loss: 0.580\n",
      "Val Accuracy is: 0.8682500123977661\n",
      "[317] Training loss: 0.575\t Validation loss: 0.580\n",
      "Val Accuracy is: 0.8682500123977661\n",
      "[318] Training loss: 0.575\t Validation loss: 0.580\n",
      "Val Accuracy is: 0.8682500123977661\n",
      "[319] Training loss: 0.575\t Validation loss: 0.580\n",
      "Val Accuracy is: 0.8682500123977661\n",
      "[320] Training loss: 0.574\t Validation loss: 0.580\n",
      "Val Accuracy is: 0.8682500123977661\n",
      "[321] Training loss: 0.574\t Validation loss: 0.579\n",
      "Val Accuracy is: 0.8682500123977661\n",
      "[322] Training loss: 0.574\t Validation loss: 0.579\n",
      "Val Accuracy is: 0.8682500123977661\n",
      "[323] Training loss: 0.574\t Validation loss: 0.579\n",
      "Val Accuracy is: 0.8680000305175781\n",
      "[324] Training loss: 0.573\t Validation loss: 0.579\n",
      "Val Accuracy is: 0.8677500486373901\n",
      "[325] Training loss: 0.573\t Validation loss: 0.578\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[326] Training loss: 0.573\t Validation loss: 0.578\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[327] Training loss: 0.573\t Validation loss: 0.578\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[328] Training loss: 0.572\t Validation loss: 0.578\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[329] Training loss: 0.572\t Validation loss: 0.577\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[330] Training loss: 0.572\t Validation loss: 0.577\n",
      "Val Accuracy is: 0.8670000433921814\n",
      "[331] Training loss: 0.572\t Validation loss: 0.577\n",
      "Val Accuracy is: 0.8670000433921814\n",
      "[332] Training loss: 0.572\t Validation loss: 0.577\n",
      "Val Accuracy is: 0.8670000433921814\n",
      "[333] Training loss: 0.571\t Validation loss: 0.577\n",
      "Val Accuracy is: 0.8670000433921814\n",
      "[334] Training loss: 0.571\t Validation loss: 0.576\n",
      "Val Accuracy is: 0.8670000433921814\n",
      "[335] Training loss: 0.571\t Validation loss: 0.576\n",
      "Val Accuracy is: 0.8670000433921814\n",
      "[336] Training loss: 0.571\t Validation loss: 0.576\n",
      "Val Accuracy is: 0.8670000433921814\n",
      "[337] Training loss: 0.570\t Validation loss: 0.576\n",
      "Val Accuracy is: 0.8672500252723694\n",
      "[338] Training loss: 0.570\t Validation loss: 0.575\n",
      "Val Accuracy is: 0.8672500252723694\n",
      "[339] Training loss: 0.570\t Validation loss: 0.575\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[340] Training loss: 0.570\t Validation loss: 0.575\n",
      "Val Accuracy is: 0.8672500252723694\n",
      "[341] Training loss: 0.570\t Validation loss: 0.575\n",
      "Val Accuracy is: 0.8672500252723694\n",
      "[342] Training loss: 0.569\t Validation loss: 0.575\n",
      "Val Accuracy is: 0.8672500252723694\n",
      "[343] Training loss: 0.569\t Validation loss: 0.574\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[344] Training loss: 0.569\t Validation loss: 0.574\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[345] Training loss: 0.569\t Validation loss: 0.574\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[346] Training loss: 0.568\t Validation loss: 0.574\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[347] Training loss: 0.568\t Validation loss: 0.573\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[348] Training loss: 0.568\t Validation loss: 0.573\n",
      "Val Accuracy is: 0.8672500252723694\n",
      "[349] Training loss: 0.568\t Validation loss: 0.573\n",
      "Val Accuracy is: 0.8672500252723694\n",
      "[350] Training loss: 0.568\t Validation loss: 0.573\n",
      "Val Accuracy is: 0.8672500252723694\n",
      "[351] Training loss: 0.567\t Validation loss: 0.573\n",
      "Val Accuracy is: 0.8672500252723694\n",
      "[352] Training loss: 0.567\t Validation loss: 0.572\n",
      "Val Accuracy is: 0.8672500252723694\n",
      "[353] Training loss: 0.567\t Validation loss: 0.572\n",
      "Val Accuracy is: 0.8672500252723694\n",
      "[354] Training loss: 0.567\t Validation loss: 0.572\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[355] Training loss: 0.566\t Validation loss: 0.572\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[356] Training loss: 0.566\t Validation loss: 0.572\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[357] Training loss: 0.566\t Validation loss: 0.571\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[358] Training loss: 0.566\t Validation loss: 0.571\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[359] Training loss: 0.566\t Validation loss: 0.571\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[360] Training loss: 0.565\t Validation loss: 0.571\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[361] Training loss: 0.565\t Validation loss: 0.570\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[362] Training loss: 0.565\t Validation loss: 0.570\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[363] Training loss: 0.565\t Validation loss: 0.570\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[364] Training loss: 0.564\t Validation loss: 0.570\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[365] Training loss: 0.564\t Validation loss: 0.570\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[366] Training loss: 0.564\t Validation loss: 0.569\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[367] Training loss: 0.564\t Validation loss: 0.569\n",
      "Val Accuracy is: 0.8675000667572021\n",
      "[368] Training loss: 0.564\t Validation loss: 0.569\n",
      "Val Accuracy is: 0.8677500486373901\n",
      "[369] Training loss: 0.563\t Validation loss: 0.569\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[370] Training loss: 0.563\t Validation loss: 0.569\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[371] Training loss: 0.563\t Validation loss: 0.568\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[372] Training loss: 0.563\t Validation loss: 0.568\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[373] Training loss: 0.563\t Validation loss: 0.568\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[374] Training loss: 0.562\t Validation loss: 0.568\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[375] Training loss: 0.562\t Validation loss: 0.567\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[376] Training loss: 0.562\t Validation loss: 0.567\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[377] Training loss: 0.562\t Validation loss: 0.567\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[378] Training loss: 0.561\t Validation loss: 0.567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy is: 0.9687500596046448\n",
      "[379] Training loss: 0.561\t Validation loss: 0.567\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[380] Training loss: 0.561\t Validation loss: 0.566\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[381] Training loss: 0.561\t Validation loss: 0.566\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[382] Training loss: 0.561\t Validation loss: 0.566\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[383] Training loss: 0.560\t Validation loss: 0.566\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[384] Training loss: 0.560\t Validation loss: 0.566\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[385] Training loss: 0.560\t Validation loss: 0.565\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[386] Training loss: 0.560\t Validation loss: 0.565\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[387] Training loss: 0.560\t Validation loss: 0.565\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[388] Training loss: 0.559\t Validation loss: 0.565\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[389] Training loss: 0.559\t Validation loss: 0.565\n",
      "Val Accuracy is: 0.968250036239624\n",
      "[390] Training loss: 0.559\t Validation loss: 0.564\n",
      "Val Accuracy is: 0.968250036239624\n",
      "[391] Training loss: 0.559\t Validation loss: 0.564\n",
      "Val Accuracy is: 0.968250036239624\n",
      "[392] Training loss: 0.559\t Validation loss: 0.564\n",
      "Val Accuracy is: 0.968250036239624\n",
      "[393] Training loss: 0.558\t Validation loss: 0.564\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[394] Training loss: 0.558\t Validation loss: 0.564\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[395] Training loss: 0.558\t Validation loss: 0.563\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[396] Training loss: 0.558\t Validation loss: 0.563\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[397] Training loss: 0.558\t Validation loss: 0.563\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[398] Training loss: 0.557\t Validation loss: 0.563\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[399] Training loss: 0.557\t Validation loss: 0.562\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[400] Training loss: 0.557\t Validation loss: 0.562\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[401] Training loss: 0.557\t Validation loss: 0.562\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[402] Training loss: 0.556\t Validation loss: 0.562\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[403] Training loss: 0.556\t Validation loss: 0.562\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[404] Training loss: 0.556\t Validation loss: 0.561\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[405] Training loss: 0.556\t Validation loss: 0.561\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[406] Training loss: 0.556\t Validation loss: 0.561\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[407] Training loss: 0.555\t Validation loss: 0.561\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[408] Training loss: 0.555\t Validation loss: 0.561\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[409] Training loss: 0.555\t Validation loss: 0.560\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[410] Training loss: 0.555\t Validation loss: 0.560\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[411] Training loss: 0.555\t Validation loss: 0.560\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[412] Training loss: 0.554\t Validation loss: 0.560\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[413] Training loss: 0.554\t Validation loss: 0.560\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[414] Training loss: 0.554\t Validation loss: 0.559\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[415] Training loss: 0.554\t Validation loss: 0.559\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[416] Training loss: 0.554\t Validation loss: 0.559\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[417] Training loss: 0.553\t Validation loss: 0.559\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[418] Training loss: 0.553\t Validation loss: 0.559\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[419] Training loss: 0.553\t Validation loss: 0.558\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[420] Training loss: 0.553\t Validation loss: 0.558\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[421] Training loss: 0.553\t Validation loss: 0.558\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[422] Training loss: 0.552\t Validation loss: 0.558\n",
      "Val Accuracy is: 0.968250036239624\n",
      "[423] Training loss: 0.552\t Validation loss: 0.558\n",
      "Val Accuracy is: 0.968250036239624\n",
      "[424] Training loss: 0.552\t Validation loss: 0.557\n",
      "Val Accuracy is: 0.968250036239624\n",
      "[425] Training loss: 0.552\t Validation loss: 0.557\n",
      "Val Accuracy is: 0.968250036239624\n",
      "[426] Training loss: 0.552\t Validation loss: 0.557\n",
      "Val Accuracy is: 0.968000054359436\n",
      "[427] Training loss: 0.551\t Validation loss: 0.557\n",
      "Val Accuracy is: 0.968000054359436\n",
      "[428] Training loss: 0.551\t Validation loss: 0.557\n",
      "Val Accuracy is: 0.968000054359436\n",
      "[429] Training loss: 0.551\t Validation loss: 0.557\n",
      "Val Accuracy is: 0.968000054359436\n",
      "[430] Training loss: 0.551\t Validation loss: 0.556\n",
      "Val Accuracy is: 0.968000054359436\n",
      "[431] Training loss: 0.551\t Validation loss: 0.556\n",
      "Val Accuracy is: 0.968000054359436\n",
      "[432] Training loss: 0.550\t Validation loss: 0.556\n",
      "Val Accuracy is: 0.968000054359436\n",
      "[433] Training loss: 0.550\t Validation loss: 0.556\n",
      "Val Accuracy is: 0.968000054359436\n",
      "[434] Training loss: 0.550\t Validation loss: 0.556\n",
      "Val Accuracy is: 0.968000054359436\n",
      "[435] Training loss: 0.550\t Validation loss: 0.555\n",
      "Val Accuracy is: 0.968250036239624\n",
      "[436] Training loss: 0.550\t Validation loss: 0.555\n",
      "Val Accuracy is: 0.968250036239624\n",
      "[437] Training loss: 0.549\t Validation loss: 0.555\n",
      "Val Accuracy is: 0.968250036239624\n",
      "[438] Training loss: 0.549\t Validation loss: 0.555\n",
      "Val Accuracy is: 0.968000054359436\n",
      "[439] Training loss: 0.549\t Validation loss: 0.555\n",
      "Val Accuracy is: 0.968000054359436\n",
      "[440] Training loss: 0.549\t Validation loss: 0.554\n",
      "Val Accuracy is: 0.968000054359436\n",
      "[441] Training loss: 0.549\t Validation loss: 0.554\n",
      "Val Accuracy is: 0.968000054359436\n",
      "[442] Training loss: 0.549\t Validation loss: 0.554\n",
      "Val Accuracy is: 0.968000054359436\n",
      "[443] Training loss: 0.548\t Validation loss: 0.554\n",
      "Val Accuracy is: 0.968000054359436\n",
      "[444] Training loss: 0.548\t Validation loss: 0.554\n",
      "Val Accuracy is: 0.968000054359436\n",
      "[445] Training loss: 0.548\t Validation loss: 0.553\n",
      "Val Accuracy is: 0.968000054359436\n",
      "[446] Training loss: 0.548\t Validation loss: 0.553\n",
      "Val Accuracy is: 0.968000054359436\n",
      "[447] Training loss: 0.548\t Validation loss: 0.553\n",
      "Val Accuracy is: 0.968000054359436\n",
      "[448] Training loss: 0.547\t Validation loss: 0.553\n",
      "Val Accuracy is: 0.968250036239624\n",
      "[449] Training loss: 0.547\t Validation loss: 0.553\n",
      "Val Accuracy is: 0.968250036239624\n",
      "[450] Training loss: 0.547\t Validation loss: 0.552\n",
      "Val Accuracy is: 0.968250036239624\n",
      "[451] Training loss: 0.547\t Validation loss: 0.552\n",
      "Val Accuracy is: 0.968250036239624\n",
      "[452] Training loss: 0.547\t Validation loss: 0.552\n",
      "Val Accuracy is: 0.968250036239624\n",
      "[453] Training loss: 0.546\t Validation loss: 0.552\n",
      "Val Accuracy is: 0.968250036239624\n",
      "[454] Training loss: 0.546\t Validation loss: 0.552\n",
      "Val Accuracy is: 0.968250036239624\n",
      "[455] Training loss: 0.546\t Validation loss: 0.552\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[456] Training loss: 0.546\t Validation loss: 0.551\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[457] Training loss: 0.546\t Validation loss: 0.551\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[458] Training loss: 0.545\t Validation loss: 0.551\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[459] Training loss: 0.545\t Validation loss: 0.551\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[460] Training loss: 0.545\t Validation loss: 0.551\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[461] Training loss: 0.545\t Validation loss: 0.550\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[462] Training loss: 0.545\t Validation loss: 0.550\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[463] Training loss: 0.545\t Validation loss: 0.550\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[464] Training loss: 0.544\t Validation loss: 0.550\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[465] Training loss: 0.544\t Validation loss: 0.550\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[466] Training loss: 0.544\t Validation loss: 0.549\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[467] Training loss: 0.544\t Validation loss: 0.549\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[468] Training loss: 0.544\t Validation loss: 0.549\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[469] Training loss: 0.543\t Validation loss: 0.549\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[470] Training loss: 0.543\t Validation loss: 0.549\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[471] Training loss: 0.543\t Validation loss: 0.549\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[472] Training loss: 0.543\t Validation loss: 0.548\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[473] Training loss: 0.543\t Validation loss: 0.548\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[474] Training loss: 0.542\t Validation loss: 0.548\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[475] Training loss: 0.542\t Validation loss: 0.548\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[476] Training loss: 0.542\t Validation loss: 0.548\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[477] Training loss: 0.542\t Validation loss: 0.547\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[478] Training loss: 0.542\t Validation loss: 0.547\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[479] Training loss: 0.542\t Validation loss: 0.547\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[480] Training loss: 0.541\t Validation loss: 0.547\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[481] Training loss: 0.541\t Validation loss: 0.547\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[482] Training loss: 0.541\t Validation loss: 0.547\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[483] Training loss: 0.541\t Validation loss: 0.546\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[484] Training loss: 0.541\t Validation loss: 0.546\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[485] Training loss: 0.540\t Validation loss: 0.546\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[486] Training loss: 0.540\t Validation loss: 0.546\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[487] Training loss: 0.540\t Validation loss: 0.546\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[488] Training loss: 0.540\t Validation loss: 0.545\n",
      "Val Accuracy is: 0.968500018119812\n",
      "[489] Training loss: 0.540\t Validation loss: 0.545\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[490] Training loss: 0.540\t Validation loss: 0.545\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[491] Training loss: 0.539\t Validation loss: 0.545\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[492] Training loss: 0.539\t Validation loss: 0.545\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[493] Training loss: 0.539\t Validation loss: 0.545\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[494] Training loss: 0.539\t Validation loss: 0.544\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[495] Training loss: 0.539\t Validation loss: 0.544\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[496] Training loss: 0.538\t Validation loss: 0.544\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[497] Training loss: 0.538\t Validation loss: 0.544\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[498] Training loss: 0.538\t Validation loss: 0.544\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[499] Training loss: 0.538\t Validation loss: 0.543\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[500] Training loss: 0.538\t Validation loss: 0.543\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[501] Training loss: 0.538\t Validation loss: 0.543\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[502] Training loss: 0.537\t Validation loss: 0.543\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[503] Training loss: 0.537\t Validation loss: 0.543\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[504] Training loss: 0.537\t Validation loss: 0.543\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[505] Training loss: 0.537\t Validation loss: 0.542\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[506] Training loss: 0.537\t Validation loss: 0.542\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[507] Training loss: 0.536\t Validation loss: 0.542\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[508] Training loss: 0.536\t Validation loss: 0.542\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[509] Training loss: 0.536\t Validation loss: 0.542\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[510] Training loss: 0.536\t Validation loss: 0.541\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[511] Training loss: 0.536\t Validation loss: 0.541\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[512] Training loss: 0.536\t Validation loss: 0.541\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[513] Training loss: 0.535\t Validation loss: 0.541\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[514] Training loss: 0.535\t Validation loss: 0.541\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[515] Training loss: 0.535\t Validation loss: 0.541\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[516] Training loss: 0.535\t Validation loss: 0.540\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[517] Training loss: 0.535\t Validation loss: 0.540\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[518] Training loss: 0.535\t Validation loss: 0.540\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[519] Training loss: 0.534\t Validation loss: 0.540\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[520] Training loss: 0.534\t Validation loss: 0.540\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[521] Training loss: 0.534\t Validation loss: 0.540\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[522] Training loss: 0.534\t Validation loss: 0.539\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[523] Training loss: 0.534\t Validation loss: 0.539\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[524] Training loss: 0.533\t Validation loss: 0.539\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[525] Training loss: 0.533\t Validation loss: 0.539\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[526] Training loss: 0.533\t Validation loss: 0.539\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[527] Training loss: 0.533\t Validation loss: 0.539\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[528] Training loss: 0.533\t Validation loss: 0.538\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[529] Training loss: 0.533\t Validation loss: 0.538\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[530] Training loss: 0.532\t Validation loss: 0.538\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[531] Training loss: 0.532\t Validation loss: 0.538\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[532] Training loss: 0.532\t Validation loss: 0.538\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[533] Training loss: 0.532\t Validation loss: 0.538\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[534] Training loss: 0.532\t Validation loss: 0.537\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[535] Training loss: 0.532\t Validation loss: 0.537\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[536] Training loss: 0.531\t Validation loss: 0.537\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[537] Training loss: 0.531\t Validation loss: 0.537\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[538] Training loss: 0.531\t Validation loss: 0.537\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[539] Training loss: 0.531\t Validation loss: 0.536\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[540] Training loss: 0.531\t Validation loss: 0.536\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[541] Training loss: 0.531\t Validation loss: 0.536\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[542] Training loss: 0.530\t Validation loss: 0.536\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[543] Training loss: 0.530\t Validation loss: 0.536\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[544] Training loss: 0.530\t Validation loss: 0.536\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[545] Training loss: 0.530\t Validation loss: 0.535\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[546] Training loss: 0.530\t Validation loss: 0.535\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[547] Training loss: 0.530\t Validation loss: 0.535\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[548] Training loss: 0.529\t Validation loss: 0.535\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[549] Training loss: 0.529\t Validation loss: 0.535\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[550] Training loss: 0.529\t Validation loss: 0.535\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[551] Training loss: 0.529\t Validation loss: 0.534\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[552] Training loss: 0.529\t Validation loss: 0.534\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[553] Training loss: 0.529\t Validation loss: 0.534\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[554] Training loss: 0.528\t Validation loss: 0.534\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[555] Training loss: 0.528\t Validation loss: 0.534\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[556] Training loss: 0.528\t Validation loss: 0.534\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[557] Training loss: 0.528\t Validation loss: 0.533\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[558] Training loss: 0.528\t Validation loss: 0.533\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[559] Training loss: 0.528\t Validation loss: 0.533\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[560] Training loss: 0.527\t Validation loss: 0.533\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[561] Training loss: 0.527\t Validation loss: 0.533\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[562] Training loss: 0.527\t Validation loss: 0.533\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[563] Training loss: 0.527\t Validation loss: 0.532\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[564] Training loss: 0.527\t Validation loss: 0.532\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[565] Training loss: 0.527\t Validation loss: 0.532\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[566] Training loss: 0.526\t Validation loss: 0.532\n",
      "Val Accuracy is: 0.9695000648498535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[567] Training loss: 0.526\t Validation loss: 0.532\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[568] Training loss: 0.526\t Validation loss: 0.532\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[569] Training loss: 0.526\t Validation loss: 0.531\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[570] Training loss: 0.526\t Validation loss: 0.531\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[571] Training loss: 0.526\t Validation loss: 0.531\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[572] Training loss: 0.525\t Validation loss: 0.531\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[573] Training loss: 0.525\t Validation loss: 0.531\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[574] Training loss: 0.525\t Validation loss: 0.531\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[575] Training loss: 0.525\t Validation loss: 0.531\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[576] Training loss: 0.525\t Validation loss: 0.530\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[577] Training loss: 0.525\t Validation loss: 0.530\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[578] Training loss: 0.524\t Validation loss: 0.530\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[579] Training loss: 0.524\t Validation loss: 0.530\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[580] Training loss: 0.524\t Validation loss: 0.530\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[581] Training loss: 0.524\t Validation loss: 0.530\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[582] Training loss: 0.524\t Validation loss: 0.529\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[583] Training loss: 0.524\t Validation loss: 0.529\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[584] Training loss: 0.523\t Validation loss: 0.529\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[585] Training loss: 0.523\t Validation loss: 0.529\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[586] Training loss: 0.523\t Validation loss: 0.529\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[587] Training loss: 0.523\t Validation loss: 0.529\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[588] Training loss: 0.523\t Validation loss: 0.528\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[589] Training loss: 0.523\t Validation loss: 0.528\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[590] Training loss: 0.522\t Validation loss: 0.528\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[591] Training loss: 0.522\t Validation loss: 0.528\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[592] Training loss: 0.522\t Validation loss: 0.528\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[593] Training loss: 0.522\t Validation loss: 0.528\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[594] Training loss: 0.522\t Validation loss: 0.527\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[595] Training loss: 0.522\t Validation loss: 0.527\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[596] Training loss: 0.521\t Validation loss: 0.527\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[597] Training loss: 0.521\t Validation loss: 0.527\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[598] Training loss: 0.521\t Validation loss: 0.527\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[599] Training loss: 0.521\t Validation loss: 0.527\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[600] Training loss: 0.521\t Validation loss: 0.527\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[601] Training loss: 0.521\t Validation loss: 0.526\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[602] Training loss: 0.521\t Validation loss: 0.526\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[603] Training loss: 0.520\t Validation loss: 0.526\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[604] Training loss: 0.520\t Validation loss: 0.526\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[605] Training loss: 0.520\t Validation loss: 0.526\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[606] Training loss: 0.520\t Validation loss: 0.526\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[607] Training loss: 0.520\t Validation loss: 0.525\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[608] Training loss: 0.520\t Validation loss: 0.525\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[609] Training loss: 0.519\t Validation loss: 0.525\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[610] Training loss: 0.519\t Validation loss: 0.525\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[611] Training loss: 0.519\t Validation loss: 0.525\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[612] Training loss: 0.519\t Validation loss: 0.525\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[613] Training loss: 0.519\t Validation loss: 0.524\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[614] Training loss: 0.519\t Validation loss: 0.524\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[615] Training loss: 0.518\t Validation loss: 0.524\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[616] Training loss: 0.518\t Validation loss: 0.524\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[617] Training loss: 0.518\t Validation loss: 0.524\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[618] Training loss: 0.518\t Validation loss: 0.524\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[619] Training loss: 0.518\t Validation loss: 0.524\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[620] Training loss: 0.518\t Validation loss: 0.523\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[621] Training loss: 0.518\t Validation loss: 0.523\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[622] Training loss: 0.517\t Validation loss: 0.523\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[623] Training loss: 0.517\t Validation loss: 0.523\n",
      "Val Accuracy is: 0.9687500596046448\n",
      "[624] Training loss: 0.517\t Validation loss: 0.523\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[625] Training loss: 0.517\t Validation loss: 0.523\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[626] Training loss: 0.517\t Validation loss: 0.522\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[627] Training loss: 0.517\t Validation loss: 0.522\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[628] Training loss: 0.516\t Validation loss: 0.522\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[629] Training loss: 0.516\t Validation loss: 0.522\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[630] Training loss: 0.516\t Validation loss: 0.522\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[631] Training loss: 0.516\t Validation loss: 0.522\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[632] Training loss: 0.516\t Validation loss: 0.522\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[633] Training loss: 0.516\t Validation loss: 0.521\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[634] Training loss: 0.516\t Validation loss: 0.521\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[635] Training loss: 0.515\t Validation loss: 0.521\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[636] Training loss: 0.515\t Validation loss: 0.521\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[637] Training loss: 0.515\t Validation loss: 0.521\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[638] Training loss: 0.515\t Validation loss: 0.521\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[639] Training loss: 0.515\t Validation loss: 0.520\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[640] Training loss: 0.515\t Validation loss: 0.520\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[641] Training loss: 0.515\t Validation loss: 0.520\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[642] Training loss: 0.514\t Validation loss: 0.520\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[643] Training loss: 0.514\t Validation loss: 0.520\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[644] Training loss: 0.514\t Validation loss: 0.520\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[645] Training loss: 0.514\t Validation loss: 0.520\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[646] Training loss: 0.514\t Validation loss: 0.519\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[647] Training loss: 0.514\t Validation loss: 0.519\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[648] Training loss: 0.513\t Validation loss: 0.519\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[649] Training loss: 0.513\t Validation loss: 0.519\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[650] Training loss: 0.513\t Validation loss: 0.519\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[651] Training loss: 0.513\t Validation loss: 0.519\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[652] Training loss: 0.513\t Validation loss: 0.519\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[653] Training loss: 0.513\t Validation loss: 0.518\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[654] Training loss: 0.513\t Validation loss: 0.518\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[655] Training loss: 0.512\t Validation loss: 0.518\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[656] Training loss: 0.512\t Validation loss: 0.518\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[657] Training loss: 0.512\t Validation loss: 0.518\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[658] Training loss: 0.512\t Validation loss: 0.518\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[659] Training loss: 0.512\t Validation loss: 0.518\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[660] Training loss: 0.512\t Validation loss: 0.517\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[661] Training loss: 0.512\t Validation loss: 0.517\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[662] Training loss: 0.511\t Validation loss: 0.517\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[663] Training loss: 0.511\t Validation loss: 0.517\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[664] Training loss: 0.511\t Validation loss: 0.517\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[665] Training loss: 0.511\t Validation loss: 0.517\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[666] Training loss: 0.511\t Validation loss: 0.516\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[667] Training loss: 0.511\t Validation loss: 0.516\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[668] Training loss: 0.510\t Validation loss: 0.516\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[669] Training loss: 0.510\t Validation loss: 0.516\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[670] Training loss: 0.510\t Validation loss: 0.516\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[671] Training loss: 0.510\t Validation loss: 0.516\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[672] Training loss: 0.510\t Validation loss: 0.516\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[673] Training loss: 0.510\t Validation loss: 0.515\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[674] Training loss: 0.510\t Validation loss: 0.515\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[675] Training loss: 0.509\t Validation loss: 0.515\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[676] Training loss: 0.509\t Validation loss: 0.515\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[677] Training loss: 0.509\t Validation loss: 0.515\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[678] Training loss: 0.509\t Validation loss: 0.515\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[679] Training loss: 0.509\t Validation loss: 0.515\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[680] Training loss: 0.509\t Validation loss: 0.514\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[681] Training loss: 0.509\t Validation loss: 0.514\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[682] Training loss: 0.508\t Validation loss: 0.514\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[683] Training loss: 0.508\t Validation loss: 0.514\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[684] Training loss: 0.508\t Validation loss: 0.514\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[685] Training loss: 0.508\t Validation loss: 0.514\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[686] Training loss: 0.508\t Validation loss: 0.514\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[687] Training loss: 0.508\t Validation loss: 0.513\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[688] Training loss: 0.508\t Validation loss: 0.513\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[689] Training loss: 0.507\t Validation loss: 0.513\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[690] Training loss: 0.507\t Validation loss: 0.513\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[691] Training loss: 0.507\t Validation loss: 0.513\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[692] Training loss: 0.507\t Validation loss: 0.513\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[693] Training loss: 0.507\t Validation loss: 0.513\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[694] Training loss: 0.507\t Validation loss: 0.512\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[695] Training loss: 0.507\t Validation loss: 0.512\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[696] Training loss: 0.506\t Validation loss: 0.512\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[697] Training loss: 0.506\t Validation loss: 0.512\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[698] Training loss: 0.506\t Validation loss: 0.512\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[699] Training loss: 0.506\t Validation loss: 0.512\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[700] Training loss: 0.506\t Validation loss: 0.512\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[701] Training loss: 0.506\t Validation loss: 0.511\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[702] Training loss: 0.506\t Validation loss: 0.511\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[703] Training loss: 0.505\t Validation loss: 0.511\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[704] Training loss: 0.505\t Validation loss: 0.511\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[705] Training loss: 0.505\t Validation loss: 0.511\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[706] Training loss: 0.505\t Validation loss: 0.511\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[707] Training loss: 0.505\t Validation loss: 0.511\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[708] Training loss: 0.505\t Validation loss: 0.511\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[709] Training loss: 0.505\t Validation loss: 0.510\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[710] Training loss: 0.504\t Validation loss: 0.510\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[711] Training loss: 0.504\t Validation loss: 0.510\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[712] Training loss: 0.504\t Validation loss: 0.510\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[713] Training loss: 0.504\t Validation loss: 0.510\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[714] Training loss: 0.504\t Validation loss: 0.510\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[715] Training loss: 0.504\t Validation loss: 0.510\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[716] Training loss: 0.504\t Validation loss: 0.509\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[717] Training loss: 0.504\t Validation loss: 0.509\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[718] Training loss: 0.503\t Validation loss: 0.509\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[719] Training loss: 0.503\t Validation loss: 0.509\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[720] Training loss: 0.503\t Validation loss: 0.509\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[721] Training loss: 0.503\t Validation loss: 0.509\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[722] Training loss: 0.503\t Validation loss: 0.509\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[723] Training loss: 0.503\t Validation loss: 0.508\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[724] Training loss: 0.503\t Validation loss: 0.508\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[725] Training loss: 0.502\t Validation loss: 0.508\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[726] Training loss: 0.502\t Validation loss: 0.508\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[727] Training loss: 0.502\t Validation loss: 0.508\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[728] Training loss: 0.502\t Validation loss: 0.508\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[729] Training loss: 0.502\t Validation loss: 0.508\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[730] Training loss: 0.502\t Validation loss: 0.507\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[731] Training loss: 0.502\t Validation loss: 0.507\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[732] Training loss: 0.501\t Validation loss: 0.507\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[733] Training loss: 0.501\t Validation loss: 0.507\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[734] Training loss: 0.501\t Validation loss: 0.507\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[735] Training loss: 0.501\t Validation loss: 0.507\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[736] Training loss: 0.501\t Validation loss: 0.507\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[737] Training loss: 0.501\t Validation loss: 0.507\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[738] Training loss: 0.501\t Validation loss: 0.506\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[739] Training loss: 0.501\t Validation loss: 0.506\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[740] Training loss: 0.500\t Validation loss: 0.506\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[741] Training loss: 0.500\t Validation loss: 0.506\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[742] Training loss: 0.500\t Validation loss: 0.506\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[743] Training loss: 0.500\t Validation loss: 0.506\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[744] Training loss: 0.500\t Validation loss: 0.506\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[745] Training loss: 0.500\t Validation loss: 0.505\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[746] Training loss: 0.500\t Validation loss: 0.505\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[747] Training loss: 0.499\t Validation loss: 0.505\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[748] Training loss: 0.499\t Validation loss: 0.505\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[749] Training loss: 0.499\t Validation loss: 0.505\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[750] Training loss: 0.499\t Validation loss: 0.505\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[751] Training loss: 0.499\t Validation loss: 0.505\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[752] Training loss: 0.499\t Validation loss: 0.505\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[753] Training loss: 0.499\t Validation loss: 0.504\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[754] Training loss: 0.498\t Validation loss: 0.504\n",
      "Val Accuracy is: 0.9697500467300415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[755] Training loss: 0.498\t Validation loss: 0.504\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[756] Training loss: 0.498\t Validation loss: 0.504\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[757] Training loss: 0.498\t Validation loss: 0.504\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[758] Training loss: 0.498\t Validation loss: 0.504\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[759] Training loss: 0.498\t Validation loss: 0.504\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[760] Training loss: 0.498\t Validation loss: 0.503\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[761] Training loss: 0.498\t Validation loss: 0.503\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[762] Training loss: 0.497\t Validation loss: 0.503\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[763] Training loss: 0.497\t Validation loss: 0.503\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[764] Training loss: 0.497\t Validation loss: 0.503\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[765] Training loss: 0.497\t Validation loss: 0.503\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[766] Training loss: 0.497\t Validation loss: 0.503\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[767] Training loss: 0.497\t Validation loss: 0.503\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[768] Training loss: 0.497\t Validation loss: 0.502\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[769] Training loss: 0.497\t Validation loss: 0.502\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[770] Training loss: 0.496\t Validation loss: 0.502\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[771] Training loss: 0.496\t Validation loss: 0.502\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[772] Training loss: 0.496\t Validation loss: 0.502\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[773] Training loss: 0.496\t Validation loss: 0.502\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[774] Training loss: 0.496\t Validation loss: 0.502\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[775] Training loss: 0.496\t Validation loss: 0.502\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[776] Training loss: 0.496\t Validation loss: 0.501\n",
      "Val Accuracy is: 0.9705000519752502\n",
      "[777] Training loss: 0.495\t Validation loss: 0.501\n",
      "Val Accuracy is: 0.9705000519752502\n",
      "[778] Training loss: 0.495\t Validation loss: 0.501\n",
      "Val Accuracy is: 0.9705000519752502\n",
      "[779] Training loss: 0.495\t Validation loss: 0.501\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[780] Training loss: 0.495\t Validation loss: 0.501\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[781] Training loss: 0.495\t Validation loss: 0.501\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[782] Training loss: 0.495\t Validation loss: 0.501\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[783] Training loss: 0.495\t Validation loss: 0.500\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[784] Training loss: 0.495\t Validation loss: 0.500\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[785] Training loss: 0.494\t Validation loss: 0.500\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[786] Training loss: 0.494\t Validation loss: 0.500\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[787] Training loss: 0.494\t Validation loss: 0.500\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[788] Training loss: 0.494\t Validation loss: 0.500\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[789] Training loss: 0.494\t Validation loss: 0.500\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[790] Training loss: 0.494\t Validation loss: 0.500\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[791] Training loss: 0.494\t Validation loss: 0.499\n",
      "Val Accuracy is: 0.9705000519752502\n",
      "[792] Training loss: 0.494\t Validation loss: 0.499\n",
      "Val Accuracy is: 0.9705000519752502\n",
      "[793] Training loss: 0.493\t Validation loss: 0.499\n",
      "Val Accuracy is: 0.9705000519752502\n",
      "[794] Training loss: 0.493\t Validation loss: 0.499\n",
      "Val Accuracy is: 0.9705000519752502\n",
      "[795] Training loss: 0.493\t Validation loss: 0.499\n",
      "Val Accuracy is: 0.9705000519752502\n",
      "[796] Training loss: 0.493\t Validation loss: 0.499\n",
      "Val Accuracy is: 0.9705000519752502\n",
      "[797] Training loss: 0.493\t Validation loss: 0.499\n",
      "Val Accuracy is: 0.9705000519752502\n",
      "[798] Training loss: 0.493\t Validation loss: 0.499\n",
      "Val Accuracy is: 0.9705000519752502\n",
      "[799] Training loss: 0.493\t Validation loss: 0.498\n",
      "Val Accuracy is: 0.9705000519752502\n",
      "[800] Training loss: 0.493\t Validation loss: 0.498\n",
      "Val Accuracy is: 0.9705000519752502\n",
      "[801] Training loss: 0.492\t Validation loss: 0.498\n",
      "Val Accuracy is: 0.9705000519752502\n",
      "[802] Training loss: 0.492\t Validation loss: 0.498\n",
      "Val Accuracy is: 0.9705000519752502\n",
      "[803] Training loss: 0.492\t Validation loss: 0.498\n",
      "Val Accuracy is: 0.9707500338554382\n",
      "[804] Training loss: 0.492\t Validation loss: 0.498\n",
      "Val Accuracy is: 0.9707500338554382\n",
      "[805] Training loss: 0.492\t Validation loss: 0.498\n",
      "Val Accuracy is: 0.9707500338554382\n",
      "[806] Training loss: 0.492\t Validation loss: 0.498\n",
      "Val Accuracy is: 0.9707500338554382\n",
      "[807] Training loss: 0.492\t Validation loss: 0.497\n",
      "Val Accuracy is: 0.9707500338554382\n",
      "[808] Training loss: 0.492\t Validation loss: 0.497\n",
      "Val Accuracy is: 0.9707500338554382\n",
      "[809] Training loss: 0.491\t Validation loss: 0.497\n",
      "Val Accuracy is: 0.9707500338554382\n",
      "[810] Training loss: 0.491\t Validation loss: 0.497\n",
      "Val Accuracy is: 0.9707500338554382\n",
      "[811] Training loss: 0.491\t Validation loss: 0.497\n",
      "Val Accuracy is: 0.9707500338554382\n",
      "[812] Training loss: 0.491\t Validation loss: 0.497\n",
      "Val Accuracy is: 0.9707500338554382\n",
      "[813] Training loss: 0.491\t Validation loss: 0.497\n",
      "Val Accuracy is: 0.9705000519752502\n",
      "[814] Training loss: 0.491\t Validation loss: 0.497\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[815] Training loss: 0.491\t Validation loss: 0.496\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[816] Training loss: 0.491\t Validation loss: 0.496\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[817] Training loss: 0.490\t Validation loss: 0.496\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[818] Training loss: 0.490\t Validation loss: 0.496\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[819] Training loss: 0.490\t Validation loss: 0.496\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[820] Training loss: 0.490\t Validation loss: 0.496\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[821] Training loss: 0.490\t Validation loss: 0.496\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[822] Training loss: 0.490\t Validation loss: 0.496\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[823] Training loss: 0.490\t Validation loss: 0.495\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[824] Training loss: 0.490\t Validation loss: 0.495\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[825] Training loss: 0.489\t Validation loss: 0.495\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[826] Training loss: 0.489\t Validation loss: 0.495\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[827] Training loss: 0.489\t Validation loss: 0.495\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[828] Training loss: 0.489\t Validation loss: 0.495\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[829] Training loss: 0.489\t Validation loss: 0.495\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[830] Training loss: 0.489\t Validation loss: 0.495\n",
      "Val Accuracy is: 0.9702500700950623\n",
      "[831] Training loss: 0.489\t Validation loss: 0.495\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[832] Training loss: 0.489\t Validation loss: 0.494\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[833] Training loss: 0.488\t Validation loss: 0.494\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[834] Training loss: 0.488\t Validation loss: 0.494\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[835] Training loss: 0.488\t Validation loss: 0.494\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[836] Training loss: 0.488\t Validation loss: 0.494\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[837] Training loss: 0.488\t Validation loss: 0.494\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[838] Training loss: 0.488\t Validation loss: 0.494\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[839] Training loss: 0.488\t Validation loss: 0.494\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[840] Training loss: 0.488\t Validation loss: 0.493\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[841] Training loss: 0.487\t Validation loss: 0.493\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[842] Training loss: 0.487\t Validation loss: 0.493\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[843] Training loss: 0.487\t Validation loss: 0.493\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[844] Training loss: 0.487\t Validation loss: 0.493\n",
      "Val Accuracy is: 0.9700000286102295\n",
      "[845] Training loss: 0.487\t Validation loss: 0.493\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[846] Training loss: 0.487\t Validation loss: 0.493\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[847] Training loss: 0.487\t Validation loss: 0.493\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[848] Training loss: 0.487\t Validation loss: 0.492\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[849] Training loss: 0.487\t Validation loss: 0.492\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[850] Training loss: 0.486\t Validation loss: 0.492\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[851] Training loss: 0.486\t Validation loss: 0.492\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[852] Training loss: 0.486\t Validation loss: 0.492\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[853] Training loss: 0.486\t Validation loss: 0.492\n",
      "Val Accuracy is: 0.9697500467300415\n",
      "[854] Training loss: 0.486\t Validation loss: 0.492\n",
      "Val Accuracy is: 0.9695000648498535\n",
      "[855] Training loss: 0.486\t Validation loss: 0.492\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[856] Training loss: 0.486\t Validation loss: 0.492\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[857] Training loss: 0.486\t Validation loss: 0.491\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[858] Training loss: 0.485\t Validation loss: 0.491\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[859] Training loss: 0.485\t Validation loss: 0.491\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[860] Training loss: 0.485\t Validation loss: 0.491\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[861] Training loss: 0.485\t Validation loss: 0.491\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[862] Training loss: 0.485\t Validation loss: 0.491\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[863] Training loss: 0.485\t Validation loss: 0.491\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[864] Training loss: 0.485\t Validation loss: 0.491\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[865] Training loss: 0.485\t Validation loss: 0.490\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[866] Training loss: 0.485\t Validation loss: 0.490\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[867] Training loss: 0.484\t Validation loss: 0.490\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[868] Training loss: 0.484\t Validation loss: 0.490\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[869] Training loss: 0.484\t Validation loss: 0.490\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[870] Training loss: 0.484\t Validation loss: 0.490\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[871] Training loss: 0.484\t Validation loss: 0.490\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[872] Training loss: 0.484\t Validation loss: 0.490\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[873] Training loss: 0.484\t Validation loss: 0.490\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[874] Training loss: 0.484\t Validation loss: 0.489\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[875] Training loss: 0.483\t Validation loss: 0.489\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[876] Training loss: 0.483\t Validation loss: 0.489\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[877] Training loss: 0.483\t Validation loss: 0.489\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[878] Training loss: 0.483\t Validation loss: 0.489\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[879] Training loss: 0.483\t Validation loss: 0.489\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[880] Training loss: 0.483\t Validation loss: 0.489\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[881] Training loss: 0.483\t Validation loss: 0.489\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[882] Training loss: 0.483\t Validation loss: 0.489\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[883] Training loss: 0.483\t Validation loss: 0.488\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[884] Training loss: 0.482\t Validation loss: 0.488\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[885] Training loss: 0.482\t Validation loss: 0.488\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[886] Training loss: 0.482\t Validation loss: 0.488\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[887] Training loss: 0.482\t Validation loss: 0.488\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[888] Training loss: 0.482\t Validation loss: 0.488\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[889] Training loss: 0.482\t Validation loss: 0.488\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[890] Training loss: 0.482\t Validation loss: 0.488\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[891] Training loss: 0.482\t Validation loss: 0.487\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[892] Training loss: 0.482\t Validation loss: 0.487\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[893] Training loss: 0.481\t Validation loss: 0.487\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[894] Training loss: 0.481\t Validation loss: 0.487\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[895] Training loss: 0.481\t Validation loss: 0.487\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[896] Training loss: 0.481\t Validation loss: 0.487\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[897] Training loss: 0.481\t Validation loss: 0.487\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[898] Training loss: 0.481\t Validation loss: 0.487\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[899] Training loss: 0.481\t Validation loss: 0.487\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[900] Training loss: 0.481\t Validation loss: 0.486\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[901] Training loss: 0.480\t Validation loss: 0.486\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[902] Training loss: 0.480\t Validation loss: 0.486\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[903] Training loss: 0.480\t Validation loss: 0.486\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[904] Training loss: 0.480\t Validation loss: 0.486\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[905] Training loss: 0.480\t Validation loss: 0.486\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[906] Training loss: 0.480\t Validation loss: 0.486\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[907] Training loss: 0.480\t Validation loss: 0.486\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[908] Training loss: 0.480\t Validation loss: 0.486\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[909] Training loss: 0.480\t Validation loss: 0.485\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[910] Training loss: 0.479\t Validation loss: 0.485\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[911] Training loss: 0.479\t Validation loss: 0.485\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[912] Training loss: 0.479\t Validation loss: 0.485\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[913] Training loss: 0.479\t Validation loss: 0.485\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[914] Training loss: 0.479\t Validation loss: 0.485\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[915] Training loss: 0.479\t Validation loss: 0.485\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[916] Training loss: 0.479\t Validation loss: 0.485\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[917] Training loss: 0.479\t Validation loss: 0.485\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[918] Training loss: 0.479\t Validation loss: 0.484\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[919] Training loss: 0.478\t Validation loss: 0.484\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[920] Training loss: 0.478\t Validation loss: 0.484\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[921] Training loss: 0.478\t Validation loss: 0.484\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[922] Training loss: 0.478\t Validation loss: 0.484\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[923] Training loss: 0.478\t Validation loss: 0.484\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[924] Training loss: 0.478\t Validation loss: 0.484\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[925] Training loss: 0.478\t Validation loss: 0.484\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[926] Training loss: 0.478\t Validation loss: 0.484\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[927] Training loss: 0.478\t Validation loss: 0.483\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[928] Training loss: 0.477\t Validation loss: 0.483\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[929] Training loss: 0.477\t Validation loss: 0.483\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[930] Training loss: 0.477\t Validation loss: 0.483\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[931] Training loss: 0.477\t Validation loss: 0.483\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[932] Training loss: 0.477\t Validation loss: 0.483\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[933] Training loss: 0.477\t Validation loss: 0.483\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[934] Training loss: 0.477\t Validation loss: 0.483\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[935] Training loss: 0.477\t Validation loss: 0.483\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[936] Training loss: 0.477\t Validation loss: 0.482\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[937] Training loss: 0.476\t Validation loss: 0.482\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[938] Training loss: 0.476\t Validation loss: 0.482\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[939] Training loss: 0.476\t Validation loss: 0.482\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[940] Training loss: 0.476\t Validation loss: 0.482\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[941] Training loss: 0.476\t Validation loss: 0.482\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[942] Training loss: 0.476\t Validation loss: 0.482\n",
      "Val Accuracy is: 0.9690000414848328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[943] Training loss: 0.476\t Validation loss: 0.482\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[944] Training loss: 0.476\t Validation loss: 0.482\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[945] Training loss: 0.476\t Validation loss: 0.482\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[946] Training loss: 0.476\t Validation loss: 0.481\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[947] Training loss: 0.475\t Validation loss: 0.481\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[948] Training loss: 0.475\t Validation loss: 0.481\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[949] Training loss: 0.475\t Validation loss: 0.481\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[950] Training loss: 0.475\t Validation loss: 0.481\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[951] Training loss: 0.475\t Validation loss: 0.481\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[952] Training loss: 0.475\t Validation loss: 0.481\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[953] Training loss: 0.475\t Validation loss: 0.481\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[954] Training loss: 0.475\t Validation loss: 0.481\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[955] Training loss: 0.475\t Validation loss: 0.480\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[956] Training loss: 0.474\t Validation loss: 0.480\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[957] Training loss: 0.474\t Validation loss: 0.480\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[958] Training loss: 0.474\t Validation loss: 0.480\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[959] Training loss: 0.474\t Validation loss: 0.480\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[960] Training loss: 0.474\t Validation loss: 0.480\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[961] Training loss: 0.474\t Validation loss: 0.480\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[962] Training loss: 0.474\t Validation loss: 0.480\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[963] Training loss: 0.474\t Validation loss: 0.480\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[964] Training loss: 0.474\t Validation loss: 0.479\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[965] Training loss: 0.473\t Validation loss: 0.479\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[966] Training loss: 0.473\t Validation loss: 0.479\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[967] Training loss: 0.473\t Validation loss: 0.479\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[968] Training loss: 0.473\t Validation loss: 0.479\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[969] Training loss: 0.473\t Validation loss: 0.479\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[970] Training loss: 0.473\t Validation loss: 0.479\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[971] Training loss: 0.473\t Validation loss: 0.479\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[972] Training loss: 0.473\t Validation loss: 0.479\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[973] Training loss: 0.473\t Validation loss: 0.479\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[974] Training loss: 0.473\t Validation loss: 0.478\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[975] Training loss: 0.472\t Validation loss: 0.478\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[976] Training loss: 0.472\t Validation loss: 0.478\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[977] Training loss: 0.472\t Validation loss: 0.478\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[978] Training loss: 0.472\t Validation loss: 0.478\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[979] Training loss: 0.472\t Validation loss: 0.478\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[980] Training loss: 0.472\t Validation loss: 0.478\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[981] Training loss: 0.472\t Validation loss: 0.478\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[982] Training loss: 0.472\t Validation loss: 0.478\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[983] Training loss: 0.472\t Validation loss: 0.478\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[984] Training loss: 0.472\t Validation loss: 0.477\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[985] Training loss: 0.471\t Validation loss: 0.477\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[986] Training loss: 0.471\t Validation loss: 0.477\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[987] Training loss: 0.471\t Validation loss: 0.477\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[988] Training loss: 0.471\t Validation loss: 0.477\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[989] Training loss: 0.471\t Validation loss: 0.477\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[990] Training loss: 0.471\t Validation loss: 0.477\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[991] Training loss: 0.471\t Validation loss: 0.477\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[992] Training loss: 0.471\t Validation loss: 0.477\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[993] Training loss: 0.471\t Validation loss: 0.476\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[994] Training loss: 0.470\t Validation loss: 0.476\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[995] Training loss: 0.470\t Validation loss: 0.476\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[996] Training loss: 0.470\t Validation loss: 0.476\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[997] Training loss: 0.470\t Validation loss: 0.476\n",
      "Val Accuracy is: 0.9690000414848328\n",
      "[998] Training loss: 0.470\t Validation loss: 0.476\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[999] Training loss: 0.470\t Validation loss: 0.476\n",
      "Val Accuracy is: 0.9692500233650208\n",
      "[1000] Training loss: 0.470\t Validation loss: 0.476\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader, val_loader = make_loader([X,h], Y, int(len(graphdatabase.graphlabels)), .2)\n",
    "training_losses, validation_losses = train_model(net, device, train_step, 1000, train_loader, val_loader)\n",
    "validation_loss = min(validation_losses)\n",
    "training_loss = training_losses[np.argmin(validation_loss)]\n",
    "# histories[n_samples] = {\"loss\": training_losses, \"val_loss\" :validation_losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
